{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/sartajbhuvaji/brain-tumor-classification-mri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from torch.utils.data import TensorDataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.ndimage import rotate as scipyrotate\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Define your network architecture (MLP)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size*4)\n",
    "        self.fc2 = nn.Linear(hidden_size*4, hidden_size*2)\n",
    "        self.fc3 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten input\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        y = F.relu(self.fc3(x))\n",
    "        z = self.fc4(y)\n",
    "        return z\n",
    "    \n",
    "\n",
    "def get_time():\n",
    "    return str(time.strftime(\"[%Y-%m-%d %H:%M:%S]\", time.localtime()))\n",
    "\n",
    "\n",
    "train_data_dir = 'Training'\n",
    "\n",
    "mean = [0.4914, 0.4822, 0.4465]\n",
    "std = [0.2023, 0.1994, 0.2010]\n",
    "transform = transforms.Compose([\n",
    "                                transforms.Resize((224, 224)),  \n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                #  transforms.Normalize(mean=mean, std=std)\n",
    "                                 ])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=train_data_dir, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "test_data_dir = 'Testing'\n",
    "test_dataset = datasets.ImageFolder(root=test_data_dir, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "sample_img=next(iter(train_loader))[0][0]\n",
    "im_size = (sample_img.shape[1], sample_img.shape[2])\n",
    "channel = 3\n",
    "num_classes =max(train_dataset.targets)+1\n",
    "epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-28 15:23:19] Epoch:  1 Loss of model:  2.3245182037353516\n",
      "[2023-08-28 15:27:38] Epoch:  20 Loss of model:  0.23126275837421417\n",
      "[2023-08-28 15:32:11] Epoch:  40 Loss of model:  0.19747164845466614\n",
      ">> Accuracy of Model trained on Training set on training set: 88.85%\n",
      ">> Accuracy of Model trained on Training set on testing set: 71.07%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from torch.utils.data import TensorDataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.ndimage import rotate as scipyrotate\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from captum.attr import IntegratedGradients\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Define your network architecture (MLP)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size*4)\n",
    "        self.fc2 = nn.Linear(hidden_size*4, hidden_size*2)\n",
    "        self.fc3 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten input\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        y = F.relu(self.fc3(x))\n",
    "        z = self.fc4(y)\n",
    "        return z\n",
    "    \n",
    "\n",
    "def get_time():\n",
    "    return str(time.strftime(\"[%Y-%m-%d %H:%M:%S]\", time.localtime()))\n",
    "\n",
    "\n",
    "train_data_dir = 'Training'\n",
    "\n",
    "mean = [0.4914, 0.4822, 0.4465]\n",
    "std = [0.2023, 0.1994, 0.2010]\n",
    "transform = transforms.Compose([\n",
    "                                transforms.Resize((224, 224)),  \n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                #  transforms.Normalize(mean=mean, std=std)\n",
    "                                 ])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=train_data_dir, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "test_data_dir = 'Testing'\n",
    "test_dataset = datasets.ImageFolder(root=test_data_dir, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "sample_img=next(iter(train_loader))[0][0]\n",
    "im_size = (sample_img.shape[1], sample_img.shape[2])\n",
    "channel = 3\n",
    "num_classes =max(train_dataset.targets)+1\n",
    "epochs = 40\n",
    "#------------------Train the Net--------------------------------\n",
    "net= MLP(input_size=channel * im_size[0] * im_size[1], hidden_size=128*2, output_size=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "# Teacher training before starting the dataset distillation process\n",
    "for epochy in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = net(data) \n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epochy==0:\n",
    "        print(get_time(), 'Epoch: ', epochy+1 , 'Loss of model: ', loss.item())\n",
    "    elif (epochy+1)%20==0:\n",
    "        print(get_time(), 'Epoch: ', epochy+1 , 'Loss of model: ', loss.item())\n",
    "\n",
    "del loss\n",
    "del output\n",
    "\n",
    "\n",
    "# make all parameters non-trainable, so as to make image_syn the only trainable parameter\n",
    "for param in list(net.parameters()):\n",
    "    param.requires_grad = False\n",
    "#---------------------------------------------------------------------------\n",
    "net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images=images.to(device)\n",
    "        labels=labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'>> Accuracy of Model trained on Training set on training set: {(100 * correct / total):.2f}%')\n",
    "\n",
    "# test the accuracy of the model on train_dataloader\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images=images.to(device)\n",
    "        labels=labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'>> Accuracy of Model trained on Training set on testing set: {(100 * correct / total):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========MLP Architecture=========\n",
      "fc1.weight 154140672\n",
      "fc1.bias 1024\n",
      "fc2.weight 524288\n",
      "fc2.bias 512\n",
      "fc3.weight 131072\n",
      "fc3.bias 256\n",
      "fc4.weight 1024\n",
      "fc4.bias 4\n"
     ]
    }
   ],
   "source": [
    "# count the number of parameters of net\n",
    "print(\"========MLP Architecture=========\")\n",
    "for name, param in net.named_parameters():\n",
    "    print(name, param.numel())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [2023-08-30 13:43:02] Epoch:  10 Loss of model:  0.0018491453872734888\n",
      "\t [2023-08-30 13:46:03] Epoch:  20 Loss of model:  0.000392454970925529\n",
      "\t [2023-08-30 13:49:10] Epoch:  30 Loss of model:  0.00015091459653275604\n",
      "\t [2023-08-30 13:51:59] Epoch:  40 Loss of model:  7.135632214259305e-05\n",
      "\t [2023-08-30 13:54:55] Epoch:  50 Loss of model:  3.732560847614271e-05\n",
      "\t>> Accuracy of Model trained on Training set on testing set: 76.14%\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from vit_pytorch import ViT\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from torch.utils.data import TensorDataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.ndimage import rotate as scipyrotate\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from captum.attr import IntegratedGradients\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import os\n",
    "\n",
    "\n",
    "def get_time():\n",
    "    return str(time.strftime(\"[%Y-%m-%d %H:%M:%S]\", time.localtime()))\n",
    "\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        # Convolutional layer with 16 output channels, 3x3 kernel, and padding of 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
    "        # Max pooling layer with 2x2 kernel and stride of 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Fully connected layer with 16 * 112 * 112 input features and 128 output features\n",
    "        self.fc1 = nn.Linear(16 * 112 * 112, 128)\n",
    "        # Fully connected layer with 128 input features and 'num_classes' output features\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layer, ReLU activation, and max pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # Reshape the feature maps for the fully connected layer\n",
    "        x = x.view(-1, 16 * 112 * 112)\n",
    "        # Apply the first fully connected layer and ReLU activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Apply the final fully connected layer without an activation function\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "im_size = (224, 224)  # Specify the image size\n",
    "\n",
    "num_classes = 4  # Change this according to the number of classes in your dataset\n",
    "num_headers=2\n",
    "net = CNN(\n",
    "    num_classes=num_classes,\n",
    ")\n",
    "\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net.to(device)\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "train_data_dir = os.path.abspath(\"./Training\")\n",
    "transform = transforms.Compose([\n",
    "                                transforms.Resize((224, 224)),  \n",
    "                                transforms.ToTensor(),\n",
    "                                # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                    transforms.Normalize(mean=mean, std=std)\n",
    "                                ])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=train_data_dir, transform=transform)\n",
    "\n",
    "\n",
    "trainloader= torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_data_dir = os.path.abspath(\"./Testing\")\n",
    "test_dataset = datasets.ImageFolder(root=test_data_dir, transform=transform)\n",
    "valloader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "# Teacher training before starting the dataset distillation process\n",
    "epochs=50\n",
    "for epochy in range(epochs):\n",
    "    running_loss=0.0\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = net(data) \n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    if (epochy+1)%10==0:\n",
    "        print('\\t',get_time(), 'Epoch: ', epochy+1 , 'Loss of model: ', running_loss/len(trainloader))\n",
    "\n",
    "net.eval()\n",
    "for param in list(net.parameters()):\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "del loss\n",
    "del data\n",
    "del output\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# test the accuracy of the model on train_dataloader\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in valloader:\n",
    "        images=images.to(device)\n",
    "        labels=labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_acc=100 * correct / total\n",
    "    print(f'\\t>> Accuracy of Model trained on Training set on testing set: {val_acc:.2f}%')\n",
    "\n",
    "\n",
    "\n",
    "del net\n",
    "del outputs\n",
    "del images\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer\n",
    "\n",
    "Original vision transformer from authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:14:45,518] A new study created in memory with name: no-name-02304979-4b86-483d-8e39-adb85dd8df6f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [2023-08-29 12:15:03] Epoch:  10 Loss of model:  0.8910382049424308\n",
      "\t [2023-08-29 12:15:21] Epoch:  20 Loss of model:  1.163801908493042\n",
      "\t [2023-08-29 12:15:40] Epoch:  30 Loss of model:  1.0422886184283666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:15:41,316] Trial 0 finished with value: 26.395939086294415 and parameters: {'dim': 32, 'depth': 4, 'num_heads': 2, 'mlp_dim': 8, 'drop_out': 0.38680173398955103, 'lr': 0.013962932579972016}. Best is trial 0 with value: 26.395939086294415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 26.40%\n",
      "\t [2023-08-29 12:15:59] Epoch:  10 Loss of model:  0.11754028818437032\n",
      "\t [2023-08-29 12:16:18] Epoch:  20 Loss of model:  0.31887421597327503\n",
      "\t [2023-08-29 12:16:36] Epoch:  30 Loss of model:  0.007627190356808049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:16:37,329] Trial 1 finished with value: 38.578680203045685 and parameters: {'dim': 64, 'depth': 4, 'num_heads': 4, 'mlp_dim': 16, 'drop_out': 0.08206483425286712, 'lr': 0.0024497735294532895}. Best is trial 1 with value: 38.578680203045685.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 38.58%\n",
      "\t [2023-08-29 12:16:55] Epoch:  10 Loss of model:  0.2657929297004427\n",
      "\t [2023-08-29 12:17:13] Epoch:  20 Loss of model:  0.042080555111169815\n",
      "\t [2023-08-29 12:17:31] Epoch:  30 Loss of model:  0.014808099982993943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:17:32,580] Trial 2 finished with value: 39.59390862944162 and parameters: {'dim': 64, 'depth': 4, 'num_heads': 4, 'mlp_dim': 32, 'drop_out': 0.16580492493155782, 'lr': 0.00041915362815225484}. Best is trial 2 with value: 39.59390862944162.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 39.59%\n",
      "\t [2023-08-29 12:17:50] Epoch:  10 Loss of model:  0.3077933979885919\n",
      "\t [2023-08-29 12:18:09] Epoch:  20 Loss of model:  0.23619832630668366\n",
      "\t [2023-08-29 12:18:27] Epoch:  30 Loss of model:  0.031842713111213276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:18:29,016] Trial 3 finished with value: 41.370558375634516 and parameters: {'dim': 64, 'depth': 4, 'num_heads': 2, 'mlp_dim': 64, 'drop_out': 0.01677296931628132, 'lr': 0.004439323809454755}. Best is trial 3 with value: 41.370558375634516.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 41.37%\n",
      "\t [2023-08-29 12:18:46] Epoch:  10 Loss of model:  0.2715182879141399\n",
      "\t [2023-08-29 12:19:04] Epoch:  20 Loss of model:  0.08396097259329897\n",
      "\t [2023-08-29 12:19:22] Epoch:  30 Loss of model:  0.03851139984492745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:19:23,397] Trial 4 finished with value: 42.131979695431475 and parameters: {'dim': 32, 'depth': 4, 'num_heads': 2, 'mlp_dim': 32, 'drop_out': 0.018145632484535032, 'lr': 0.006751847835325914}. Best is trial 4 with value: 42.131979695431475.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 42.13%\n",
      "\t [2023-08-29 12:19:41] Epoch:  10 Loss of model:  0.40071181101458414\n",
      "\t [2023-08-29 12:19:59] Epoch:  20 Loss of model:  0.10292925845299448\n",
      "\t [2023-08-29 12:20:17] Epoch:  30 Loss of model:  0.03175638083900724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:20:19,042] Trial 5 finished with value: 38.3248730964467 and parameters: {'dim': 64, 'depth': 4, 'num_heads': 2, 'mlp_dim': 128, 'drop_out': 0.19377663586518268, 'lr': 0.002379789059220436}. Best is trial 4 with value: 42.131979695431475.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 38.32%\n",
      "\t [2023-08-29 12:20:36] Epoch:  10 Loss of model:  0.6226784714630672\n",
      "\t [2023-08-29 12:20:54] Epoch:  20 Loss of model:  0.18143469840288162\n",
      "\t [2023-08-29 12:21:11] Epoch:  30 Loss of model:  0.08012711895363671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:21:13,011] Trial 6 finished with value: 39.59390862944162 and parameters: {'dim': 64, 'depth': 2, 'num_heads': 2, 'mlp_dim': 8, 'drop_out': 0.17645522058192717, 'lr': 0.000171121601421468}. Best is trial 4 with value: 42.131979695431475.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 39.59%\n",
      "\t [2023-08-29 12:21:30] Epoch:  10 Loss of model:  0.4201744667121342\n",
      "\t [2023-08-29 12:21:49] Epoch:  20 Loss of model:  0.13669539455856597\n",
      "\t [2023-08-29 12:22:06] Epoch:  30 Loss of model:  0.07621664500662259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:22:08,252] Trial 7 finished with value: 39.84771573604061 and parameters: {'dim': 32, 'depth': 4, 'num_heads': 4, 'mlp_dim': 16, 'drop_out': 0.020901432992158558, 'lr': 0.000142152862789402}. Best is trial 4 with value: 42.131979695431475.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 39.85%\n",
      "\t [2023-08-29 12:22:26] Epoch:  10 Loss of model:  0.16798313494239533\n",
      "\t [2023-08-29 12:22:44] Epoch:  20 Loss of model:  0.011524176424635308\n",
      "\t [2023-08-29 12:23:01] Epoch:  30 Loss of model:  0.03203709752831076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:23:02,763] Trial 8 finished with value: 41.370558375634516 and parameters: {'dim': 64, 'depth': 2, 'num_heads': 4, 'mlp_dim': 16, 'drop_out': 0.012491341817203495, 'lr': 0.0017729448354498274}. Best is trial 4 with value: 42.131979695431475.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 41.37%\n",
      "\t [2023-08-29 12:23:20] Epoch:  10 Loss of model:  0.9347388829503741\n",
      "\t [2023-08-29 12:23:39] Epoch:  20 Loss of model:  0.7509915317807879\n",
      "\t [2023-08-29 12:23:56] Epoch:  30 Loss of model:  0.7809380803789411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:23:58,319] Trial 9 finished with value: 31.218274111675125 and parameters: {'dim': 64, 'depth': 4, 'num_heads': 4, 'mlp_dim': 256, 'drop_out': 0.021701229673291984, 'lr': 0.017344626271624967}. Best is trial 4 with value: 42.131979695431475.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 31.22%\n",
      "\t [2023-08-29 12:24:16] Epoch:  10 Loss of model:  1.2084662914276123\n",
      "\t [2023-08-29 12:24:33] Epoch:  20 Loss of model:  1.4197685037340437\n",
      "\t [2023-08-29 12:24:51] Epoch:  30 Loss of model:  1.0353964482034956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:24:52,667] Trial 10 finished with value: 26.395939086294415 and parameters: {'dim': 32, 'depth': 2, 'num_heads': 2, 'mlp_dim': 32, 'drop_out': 0.03091595839447631, 'lr': 0.07076988170674581}. Best is trial 4 with value: 42.131979695431475.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 26.40%\n",
      "\t [2023-08-29 12:25:10] Epoch:  10 Loss of model:  0.7227101581437247\n",
      "\t [2023-08-29 12:25:28] Epoch:  20 Loss of model:  0.24877226778439113\n",
      "\t [2023-08-29 12:25:46] Epoch:  30 Loss of model:  0.14580167617116654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:25:48,081] Trial 11 finished with value: 40.609137055837564 and parameters: {'dim': 32, 'depth': 4, 'num_heads': 2, 'mlp_dim': 64, 'drop_out': 0.012240357364317526, 'lr': 0.007914597323441823}. Best is trial 4 with value: 42.131979695431475.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 40.61%\n",
      "\t [2023-08-29 12:26:06] Epoch:  10 Loss of model:  0.41664265947682516\n",
      "\t [2023-08-29 12:26:24] Epoch:  20 Loss of model:  0.3134544598204749\n",
      "\t [2023-08-29 12:26:42] Epoch:  30 Loss of model:  0.10282820649445057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:26:43,328] Trial 12 finished with value: 36.80203045685279 and parameters: {'dim': 32, 'depth': 4, 'num_heads': 2, 'mlp_dim': 64, 'drop_out': 0.03908094042372667, 'lr': 0.007171220683930963}. Best is trial 4 with value: 42.131979695431475.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 36.80%\n",
      "\t [2023-08-29 12:27:01] Epoch:  10 Loss of model:  0.4171803593635559\n",
      "\t [2023-08-29 12:27:19] Epoch:  20 Loss of model:  0.10181074057306562\n",
      "\t [2023-08-29 12:27:37] Epoch:  30 Loss of model:  0.019857885848198618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:27:38,761] Trial 13 finished with value: 44.16243654822335 and parameters: {'dim': 32, 'depth': 4, 'num_heads': 2, 'mlp_dim': 64, 'drop_out': 0.010263386121226712, 'lr': 0.0008680798340466958}. Best is trial 13 with value: 44.16243654822335.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 44.16%\n",
      "\t [2023-08-29 12:27:56] Epoch:  10 Loss of model:  0.2676334423678262\n",
      "\t [2023-08-29 12:28:14] Epoch:  20 Loss of model:  0.03059024443583829\n",
      "\t [2023-08-29 12:28:31] Epoch:  30 Loss of model:  0.02848436244364296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:28:32,613] Trial 14 finished with value: 42.38578680203046 and parameters: {'dim': 32, 'depth': 2, 'num_heads': 2, 'mlp_dim': 32, 'drop_out': 0.014083193583866437, 'lr': 0.0012443839220245515}. Best is trial 13 with value: 44.16243654822335.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 42.39%\n",
      "\t [2023-08-29 12:28:50] Epoch:  10 Loss of model:  0.3786285562174661\n",
      "\t [2023-08-29 12:29:08] Epoch:  20 Loss of model:  0.07785705955965179\n",
      "\t [2023-08-29 12:29:26] Epoch:  30 Loss of model:  0.020261722882943495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:29:27,777] Trial 15 finished with value: 40.609137055837564 and parameters: {'dim': 32, 'depth': 2, 'num_heads': 2, 'mlp_dim': 128, 'drop_out': 0.010248945512238847, 'lr': 0.0009693388861420829}. Best is trial 13 with value: 44.16243654822335.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 40.61%\n",
      "\t [2023-08-29 12:29:45] Epoch:  10 Loss of model:  0.2926894277334213\n",
      "\t [2023-08-29 12:30:03] Epoch:  20 Loss of model:  0.060432691659246175\n",
      "\t [2023-08-29 12:30:21] Epoch:  30 Loss of model:  0.031987963510411124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:30:22,557] Trial 16 finished with value: 39.59390862944162 and parameters: {'dim': 32, 'depth': 2, 'num_heads': 2, 'mlp_dim': 256, 'drop_out': 0.03676626084251751, 'lr': 0.0007923566345298715}. Best is trial 13 with value: 44.16243654822335.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 39.59%\n",
      "\t [2023-08-29 12:30:40] Epoch:  10 Loss of model:  0.47675961681774687\n",
      "\t [2023-08-29 12:30:57] Epoch:  20 Loss of model:  0.18114537000656128\n",
      "\t [2023-08-29 12:31:14] Epoch:  30 Loss of model:  0.04681523729647909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:31:15,572] Trial 17 finished with value: 40.609137055837564 and parameters: {'dim': 32, 'depth': 2, 'num_heads': 2, 'mlp_dim': 64, 'drop_out': 0.052032978614369556, 'lr': 0.00040900551684850664}. Best is trial 13 with value: 44.16243654822335.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 40.61%\n",
      "\t [2023-08-29 12:31:32] Epoch:  10 Loss of model:  0.222977591412408\n",
      "\t [2023-08-29 12:31:50] Epoch:  20 Loss of model:  0.23560340915407454\n",
      "\t [2023-08-29 12:32:07] Epoch:  30 Loss of model:  0.015226408573133605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:32:08,551] Trial 18 finished with value: 41.11675126903553 and parameters: {'dim': 32, 'depth': 2, 'num_heads': 2, 'mlp_dim': 32, 'drop_out': 0.010587577639880183, 'lr': 0.0013533489910347626}. Best is trial 13 with value: 44.16243654822335.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 41.12%\n",
      "\t [2023-08-29 12:32:25] Epoch:  10 Loss of model:  0.27789953351020813\n",
      "\t [2023-08-29 12:32:42] Epoch:  20 Loss of model:  0.05570588154452188\n",
      "\t [2023-08-29 12:33:00] Epoch:  30 Loss of model:  0.08912311361304351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:33:01,841] Trial 19 finished with value: 42.38578680203046 and parameters: {'dim': 32, 'depth': 2, 'num_heads': 2, 'mlp_dim': 64, 'drop_out': 0.027191110727677152, 'lr': 0.0005500866526970832}. Best is trial 13 with value: 44.16243654822335.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 42.39%\n",
      "\t [2023-08-29 12:33:19] Epoch:  10 Loss of model:  0.37741781984056744\n",
      "\t [2023-08-29 12:33:36] Epoch:  20 Loss of model:  0.1016503529889243\n",
      "\t [2023-08-29 12:33:54] Epoch:  30 Loss of model:  0.0622419108237539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:33:55,487] Trial 20 finished with value: 46.192893401015226 and parameters: {'dim': 32, 'depth': 2, 'num_heads': 2, 'mlp_dim': 32, 'drop_out': 0.019124017956274296, 'lr': 0.00030051845934512427}. Best is trial 20 with value: 46.192893401015226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 46.19%\n",
      "\t [2023-08-29 12:34:12] Epoch:  10 Loss of model:  0.7246211000851223\n",
      "\t [2023-08-29 12:34:31] Epoch:  20 Loss of model:  0.10967890173196793\n",
      "\t [2023-08-29 12:34:48] Epoch:  30 Loss of model:  0.05981253832578659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:34:50,448] Trial 21 finished with value: 42.38578680203046 and parameters: {'dim': 32, 'depth': 2, 'num_heads': 2, 'mlp_dim': 32, 'drop_out': 0.01571113756593898, 'lr': 0.00029909803738732336}. Best is trial 20 with value: 46.192893401015226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 42.39%\n",
      "\t [2023-08-29 12:35:08] Epoch:  10 Loss of model:  0.25941698040281025\n",
      "\t [2023-08-29 12:35:25] Epoch:  20 Loss of model:  0.051286239975265095\n",
      "\t [2023-08-29 12:35:43] Epoch:  30 Loss of model:  0.02518707061452525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:35:44,948] Trial 22 finished with value: 39.34010152284264 and parameters: {'dim': 32, 'depth': 2, 'num_heads': 2, 'mlp_dim': 32, 'drop_out': 0.014595584709243282, 'lr': 0.0008722113083554592}. Best is trial 20 with value: 46.192893401015226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 39.34%\n",
      "\t [2023-08-29 12:36:02] Epoch:  10 Loss of model:  0.5840792059898376\n",
      "\t [2023-08-29 12:36:20] Epoch:  20 Loss of model:  0.09738241348947797\n",
      "\t [2023-08-29 12:36:38] Epoch:  30 Loss of model:  0.04774062069399016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:36:39,738] Trial 23 finished with value: 44.41624365482234 and parameters: {'dim': 32, 'depth': 2, 'num_heads': 2, 'mlp_dim': 32, 'drop_out': 0.023254553909508244, 'lr': 0.00020632169024108706}. Best is trial 20 with value: 46.192893401015226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 44.42%\n",
      "\t [2023-08-29 12:36:57] Epoch:  10 Loss of model:  1.0165181415421622\n",
      "\t [2023-08-29 12:37:15] Epoch:  20 Loss of model:  0.399826956646783\n",
      "\t [2023-08-29 12:37:32] Epoch:  30 Loss of model:  0.14228622721774237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:37:33,912] Trial 24 finished with value: 44.16243654822335 and parameters: {'dim': 32, 'depth': 2, 'num_heads': 2, 'mlp_dim': 32, 'drop_out': 0.02399342798039473, 'lr': 0.00011327350791468061}. Best is trial 20 with value: 46.192893401015226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 44.16%\n",
      "\t [2023-08-29 12:37:51] Epoch:  10 Loss of model:  0.32862487861088346\n",
      "\t [2023-08-29 12:38:09] Epoch:  20 Loss of model:  0.06649582779833249\n",
      "\t [2023-08-29 12:38:27] Epoch:  30 Loss of model:  0.037014298673186986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:38:28,630] Trial 25 finished with value: 43.14720812182741 and parameters: {'dim': 32, 'depth': 2, 'num_heads': 2, 'mlp_dim': 256, 'drop_out': 0.010018127246391164, 'lr': 0.00026936339166249725}. Best is trial 20 with value: 46.192893401015226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 43.15%\n",
      "\t [2023-08-29 12:38:46] Epoch:  10 Loss of model:  0.5118140961442675\n",
      "\t [2023-08-29 12:39:04] Epoch:  20 Loss of model:  0.15711388736963272\n",
      "\t [2023-08-29 12:39:22] Epoch:  30 Loss of model:  0.06638357841542789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:39:24,200] Trial 26 finished with value: 39.84771573604061 and parameters: {'dim': 32, 'depth': 2, 'num_heads': 4, 'mlp_dim': 8, 'drop_out': 0.0199695573802284, 'lr': 0.00019282534739288975}. Best is trial 20 with value: 46.192893401015226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 39.85%\n",
      "\t [2023-08-29 12:39:41] Epoch:  10 Loss of model:  1.027403576033456\n",
      "\t [2023-08-29 12:39:59] Epoch:  20 Loss of model:  0.397626063653401\n",
      "\t [2023-08-29 12:40:17] Epoch:  30 Loss of model:  0.15939424293381826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:40:18,578] Trial 27 finished with value: 42.63959390862944 and parameters: {'dim': 32, 'depth': 2, 'num_heads': 2, 'mlp_dim': 128, 'drop_out': 0.026735262033604445, 'lr': 0.00010191332986404003}. Best is trial 20 with value: 46.192893401015226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 42.64%\n",
      "\t [2023-08-29 12:40:36] Epoch:  10 Loss of model:  0.3726654733930315\n",
      "\t [2023-08-29 12:40:55] Epoch:  20 Loss of model:  0.07708112789051873\n",
      "\t [2023-08-29 12:41:13] Epoch:  30 Loss of model:  0.045215675341231484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:41:14,732] Trial 28 finished with value: 41.370558375634516 and parameters: {'dim': 32, 'depth': 4, 'num_heads': 2, 'mlp_dim': 64, 'drop_out': 0.016318197049268193, 'lr': 0.0002655637569052225}. Best is trial 20 with value: 46.192893401015226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 41.37%\n",
      "\t [2023-08-29 12:41:32] Epoch:  10 Loss of model:  0.4565106545175825\n",
      "\t [2023-08-29 12:41:50] Epoch:  20 Loss of model:  0.06339795302067484\n",
      "\t [2023-08-29 12:42:08] Epoch:  30 Loss of model:  0.03066963649221829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:42:10,443] Trial 29 finished with value: 40.86294416243655 and parameters: {'dim': 32, 'depth': 4, 'num_heads': 2, 'mlp_dim': 8, 'drop_out': 0.03704294872948633, 'lr': 0.000588328550242044}. Best is trial 20 with value: 46.192893401015226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 40.86%\n",
      "\t [2023-08-29 12:42:28] Epoch:  10 Loss of model:  0.5393482616969517\n",
      "\t [2023-08-29 12:42:45] Epoch:  20 Loss of model:  0.10744363388844899\n",
      "\t [2023-08-29 12:43:03] Epoch:  30 Loss of model:  0.0561885450567518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:43:04,697] Trial 30 finished with value: 42.38578680203046 and parameters: {'dim': 32, 'depth': 2, 'num_heads': 2, 'mlp_dim': 32, 'drop_out': 0.013421659419633396, 'lr': 0.00022207989079120474}. Best is trial 20 with value: 46.192893401015226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 42.39%\n",
      "\t [2023-08-29 12:43:22] Epoch:  10 Loss of model:  0.8609678404671806\n",
      "\t [2023-08-29 12:43:39] Epoch:  20 Loss of model:  0.3070223054715565\n",
      "\t [2023-08-29 12:43:57] Epoch:  30 Loss of model:  0.10217441512005669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:43:59,324] Trial 31 finished with value: 36.54822335025381 and parameters: {'dim': 32, 'depth': 2, 'num_heads': 2, 'mlp_dim': 32, 'drop_out': 0.024105733475558555, 'lr': 0.000128788305831859}. Best is trial 20 with value: 46.192893401015226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 36.55%\n",
      "\t [2023-08-29 12:44:16] Epoch:  10 Loss of model:  0.8666230099541801\n",
      "\t [2023-08-29 12:44:34] Epoch:  20 Loss of model:  0.2702204648937498\n",
      "\t [2023-08-29 12:44:51] Epoch:  30 Loss of model:  0.07740969955921173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:44:53,305] Trial 32 finished with value: 43.65482233502538 and parameters: {'dim': 32, 'depth': 2, 'num_heads': 2, 'mlp_dim': 32, 'drop_out': 0.02138613601930017, 'lr': 0.00011339676863361597}. Best is trial 20 with value: 46.192893401015226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 43.65%\n",
      "\t [2023-08-29 12:45:11] Epoch:  10 Loss of model:  0.5954848315034594\n",
      "\t [2023-08-29 12:45:29] Epoch:  20 Loss of model:  0.11017372565610069\n",
      "\t [2023-08-29 12:45:46] Epoch:  30 Loss of model:  0.057766903191804886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:45:48,310] Trial 33 finished with value: 42.63959390862944 and parameters: {'dim': 32, 'depth': 2, 'num_heads': 2, 'mlp_dim': 32, 'drop_out': 0.05823106281603419, 'lr': 0.00036735952096944645}. Best is trial 20 with value: 46.192893401015226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 42.64%\n",
      "\t [2023-08-29 12:46:06] Epoch:  10 Loss of model:  0.438943726675851\n",
      "\t [2023-08-29 12:46:23] Epoch:  20 Loss of model:  0.11997465470007487\n",
      "\t [2023-08-29 12:46:41] Epoch:  30 Loss of model:  0.0726797633937427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:46:42,843] Trial 34 finished with value: 40.35532994923858 and parameters: {'dim': 32, 'depth': 2, 'num_heads': 4, 'mlp_dim': 32, 'drop_out': 0.017110156748918876, 'lr': 0.0002093423735335524}. Best is trial 20 with value: 46.192893401015226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 40.36%\n",
      "\t [2023-08-29 12:47:00] Epoch:  10 Loss of model:  0.4761734902858734\n",
      "\t [2023-08-29 12:47:18] Epoch:  20 Loss of model:  0.06023712349789483\n",
      "\t [2023-08-29 12:47:36] Epoch:  30 Loss of model:  0.027203680149146488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:47:37,944] Trial 35 finished with value: 41.878172588832484 and parameters: {'dim': 32, 'depth': 4, 'num_heads': 2, 'mlp_dim': 16, 'drop_out': 0.02545537190835808, 'lr': 0.0004412392174763739}. Best is trial 20 with value: 46.192893401015226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 41.88%\n",
      "\t [2023-08-29 12:47:55] Epoch:  10 Loss of model:  0.28937276346342905\n",
      "\t [2023-08-29 12:48:13] Epoch:  20 Loss of model:  0.07107339533311981\n",
      "\t [2023-08-29 12:48:32] Epoch:  30 Loss of model:  0.02381871161716325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:48:33,381] Trial 36 finished with value: 41.11675126903553 and parameters: {'dim': 64, 'depth': 4, 'num_heads': 2, 'mlp_dim': 32, 'drop_out': 0.01827068137038852, 'lr': 0.00016675968535045078}. Best is trial 20 with value: 46.192893401015226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 41.12%\n",
      "\t [2023-08-29 12:48:51] Epoch:  10 Loss of model:  0.8980795400483268\n",
      "\t [2023-08-29 12:49:08] Epoch:  20 Loss of model:  0.27065287956169676\n",
      "\t [2023-08-29 12:49:26] Epoch:  30 Loss of model:  0.10021897831133433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:49:27,607] Trial 37 finished with value: 41.878172588832484 and parameters: {'dim': 32, 'depth': 2, 'num_heads': 2, 'mlp_dim': 64, 'drop_out': 0.03139033915924924, 'lr': 0.00010176845689529392}. Best is trial 20 with value: 46.192893401015226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 41.88%\n",
      "\t [2023-08-29 12:49:45] Epoch:  10 Loss of model:  0.17633504952703202\n",
      "\t [2023-08-29 12:50:03] Epoch:  20 Loss of model:  0.02522521944982665\n",
      "\t [2023-08-29 12:50:21] Epoch:  30 Loss of model:  0.01215252439890589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:50:23,055] Trial 38 finished with value: 41.6243654822335 and parameters: {'dim': 64, 'depth': 4, 'num_heads': 4, 'mlp_dim': 32, 'drop_out': 0.012045294472843568, 'lr': 0.0002929181658528998}. Best is trial 20 with value: 46.192893401015226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 41.62%\n",
      "\t [2023-08-29 12:50:41] Epoch:  10 Loss of model:  0.7521351235253471\n",
      "\t [2023-08-29 12:50:59] Epoch:  20 Loss of model:  0.1675166870866503\n",
      "\t [2023-08-29 12:51:16] Epoch:  30 Loss of model:  0.07664497941732407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-29 12:51:18,233] Trial 39 finished with value: 44.16243654822335 and parameters: {'dim': 32, 'depth': 2, 'num_heads': 2, 'mlp_dim': 128, 'drop_out': 0.018463816773958135, 'lr': 0.0001619395223903985}. Best is trial 20 with value: 46.192893401015226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 44.16%\n",
      "Number of finished trials:  40\n",
      "Best trial:\n",
      "Value:  46.192893401015226\n",
      "Params: \n",
      "    dim: 32\n",
      "    depth: 2\n",
      "    num_heads: 2\n",
      "    mlp_dim: 32\n",
      "    drop_out: 0.019124017956274296\n",
      "    lr: 0.00030051845934512427\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from vit_pytorch import ViT\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from torch.utils.data import TensorDataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.ndimage import rotate as scipyrotate\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from captum.attr import IntegratedGradients\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import os\n",
    "\n",
    "\n",
    "def get_time():\n",
    "    return str(time.strftime(\"[%Y-%m-%d %H:%M:%S]\", time.localtime()))\n",
    "\n",
    "\n",
    "\n",
    "#============== Hyperparamer tuning for ViT =================\n",
    "# Define the training function\n",
    "def objective(trial):\n",
    "    im_size = (224, 224)  # Specify the image size\n",
    "\n",
    "    num_classes = 4  # Change this according to the number of classes in your dataset\n",
    "    num_headers=2\n",
    "    net = ViT(\n",
    "        image_size=im_size[0],\n",
    "        patch_size=56,\n",
    "        num_classes=num_classes,\n",
    "        dim=trial.suggest_categorical(\"dim\", [2 ** i for i in range(5, 7)]),\n",
    "        depth=trial.suggest_categorical(\"depth\", [2*i for i in range(1,3)]),\n",
    "        heads= trial.suggest_categorical(\"num_heads\", [num_headers*i for i in range(1, 3)]),\n",
    "        mlp_dim=trial.suggest_categorical(\"mlp_dim\", [2 ** i for i in range(3, 9)]),\n",
    "        dropout=trial.suggest_loguniform(\"drop_out\", 0.01, 0.5),\n",
    "        emb_dropout=0.0,\n",
    "        pool='cls'\n",
    "    )\n",
    "\n",
    "    device=torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    train_data_dir = os.path.abspath(\"./Training\")\n",
    "    transform = transforms.Compose([\n",
    "                                    transforms.Resize((224, 224)),  \n",
    "                                    transforms.ToTensor(),\n",
    "                                    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                     transforms.Normalize(mean=mean, std=std)\n",
    "                                    ])\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(root=train_data_dir, transform=transform)\n",
    "\n",
    "\n",
    "\n",
    "    # Create a subset of the dataset with only 10 samples per class\n",
    "    N_p_c=100\n",
    "    subset_indices = []\n",
    "    for class_idx in range(len(train_dataset.classes)):\n",
    "        class_indices = [idx for idx, (_, label) in enumerate(train_dataset.imgs) if label == class_idx]\n",
    "        subset_indices.extend(class_indices[:N_p_c])  # Sample 10 images per class\n",
    "\n",
    "    train_subset = Subset(train_dataset, subset_indices)\n",
    "    trainloader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "\n",
    "    # trainloader= torch.utils.data.DataLoader(train_dataset, batch_size=128*2, shuffle=True)\n",
    "\n",
    "    test_data_dir = os.path.abspath(\"./Testing\")\n",
    "    test_dataset = datasets.ImageFolder(root=test_data_dir, transform=transform)\n",
    "    valloader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "    net.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=trial.suggest_loguniform(\"lr\", 1e-4, 1e-1))\n",
    "\n",
    "    # Teacher training before starting the dataset distillation process\n",
    "    epochs=30\n",
    "    for epochy in range(epochs):\n",
    "        running_loss=0.0\n",
    "        for batch_idx, (data, target) in enumerate(trainloader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = net(data) \n",
    "            loss = criterion(output, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        if (epochy+1)%10==0:\n",
    "            print('\\t',get_time(), 'Epoch: ', epochy+1 , 'Loss of model: ', running_loss/len(trainloader))\n",
    "\n",
    "    net.eval()\n",
    "    for param in list(net.parameters()):\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "    del loss\n",
    "    del data\n",
    "    del output\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # test the accuracy of the model on train_dataloader\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in valloader:\n",
    "            images=images.to(device)\n",
    "            labels=labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_acc=100 * correct / total\n",
    "        print(f'\\t>> Accuracy of Model trained on Training set on testing set: {val_acc:.2f}%')\n",
    "\n",
    "\n",
    "\n",
    "    del net\n",
    "    del outputs\n",
    "    del images\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return val_acc\n",
    "\n",
    "\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=40)  # You can adjust the number of trials\n",
    "\n",
    "# Print the best trial\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"Value: \", trial.value)\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [2023-08-29 13:00:22] Epoch:  10 Loss of model:  0.2734017180899779\n",
      "\t [2023-08-29 13:02:32] Epoch:  20 Loss of model:  0.08274168616367711\n",
      "\t [2023-08-29 13:04:43] Epoch:  30 Loss of model:  0.07401238263377713\n",
      "\t>> Accuracy of Model trained on Training set on testing set: 72.59%\n"
     ]
    }
   ],
   "source": [
    "im_size = (224, 224)  # Specify the image size\n",
    "\n",
    "num_classes = 4  # Change this according to the number of classes in your dataset\n",
    "num_headers=2\n",
    "net = ViT(\n",
    "    image_size=im_size[0],\n",
    "    patch_size=56,\n",
    "    num_classes=num_classes,\n",
    "    dim=trial.params[\"dim\"],\n",
    "    depth=trial.params[\"depth\"],\n",
    "    heads=trial.params[\"num_heads\"],\n",
    "    mlp_dim=trial.params[\"mlp_dim\"],\n",
    "    dropout=trial.params[\"drop_out\"],\n",
    "    emb_dropout=0.0,\n",
    "    pool='cls'\n",
    ")\n",
    "\n",
    "device=torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "net.to(device)\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "train_data_dir = os.path.abspath(\"./Training\")\n",
    "transform = transforms.Compose([\n",
    "                                transforms.Resize((224, 224)),  \n",
    "                                transforms.ToTensor(),\n",
    "                                # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                    transforms.Normalize(mean=mean, std=std)\n",
    "                                ])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=train_data_dir, transform=transform)\n",
    "\n",
    "\n",
    "trainloader= torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_data_dir = os.path.abspath(\"./Testing\")\n",
    "test_dataset = datasets.ImageFolder(root=test_data_dir, transform=transform)\n",
    "valloader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=trial.params[\"lr\"])\n",
    "\n",
    "# Teacher training before starting the dataset distillation process\n",
    "epochs=30\n",
    "for epochy in range(epochs):\n",
    "    running_loss=0.0\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = net(data) \n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    if (epochy+1)%10==0:\n",
    "        print('\\t',get_time(), 'Epoch: ', epochy+1 , 'Loss of model: ', running_loss/len(trainloader))\n",
    "\n",
    "net.eval()\n",
    "for param in list(net.parameters()):\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "del loss\n",
    "del data\n",
    "del output\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# test the accuracy of the model on train_dataloader\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in valloader:\n",
    "        images=images.to(device)\n",
    "        labels=labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_acc=100 * correct / total\n",
    "    print(f'\\t>> Accuracy of Model trained on Training set on testing set: {val_acc:.2f}%')\n",
    "\n",
    "\n",
    "\n",
    "del net\n",
    "del outputs\n",
    "del images\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Vision Transformer\n",
    "\n",
    "An update from some of the same authors of the original paper proposes simplifications to ViT that allows it to train faster and better.\n",
    "\n",
    "Among these simplifications include 2d sinusoidal positional embedding, global average pooling (no CLS token), no dropout, batch sizes of 1024 rather than 4096, and use of RandAugment and MixUp augmentations. They also show that a simple linear at the end is not significantly worse than the original MLP head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-03 13:12:02] Epoch:  1 Loss of model:  1.3234524726867676\n",
      "[2023-12-03 13:16:18] Epoch:  20 Loss of model:  0.7982242107391357\n",
      "[2023-12-03 13:20:50] Epoch:  40 Loss of model:  0.6601513028144836\n",
      "[2023-12-03 13:25:53] Epoch:  60 Loss of model:  0.34663447737693787\n",
      "[2023-12-03 13:30:51] Epoch:  80 Loss of model:  0.13117243349552155\n",
      ">> Accuracy of Model trained on Training set on training set: 94.63%\n",
      ">> Accuracy of Model trained on Training set on testing set: 64.21%\n"
     ]
    }
   ],
   "source": [
    "from vit_pytorch import SimpleViT\n",
    "\n",
    "epochs= 80\n",
    "\n",
    "simple_vision_net = SimpleViT(\n",
    "    image_size = im_size[0],\n",
    "    patch_size = 32,\n",
    "    num_classes = num_classes,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "simple_vision_net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(simple_vision_net.parameters(), lr=1e-3)\n",
    "\n",
    "# Teacher training before starting the dataset distillation process\n",
    "for epochy in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = simple_vision_net(data) \n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epochy==0:\n",
    "        print(get_time(), 'Epoch: ', epochy+1 , 'Loss of model: ', loss.item())\n",
    "    elif (epochy+1)%20==0:\n",
    "        print(get_time(), 'Epoch: ', epochy+1 , 'Loss of model: ', loss.item())\n",
    "\n",
    "del loss\n",
    "del output\n",
    "\n",
    "\n",
    "# make all parameters non-trainable, so as to make image_syn the only trainable parameter\n",
    "for param in list(simple_vision_net.parameters()):\n",
    "    param.requires_grad = False\n",
    "#---------------------------------------------------------------------------\n",
    "simple_vision_net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images=images.to(device)\n",
    "        labels=labels.to(device)\n",
    "        outputs = simple_vision_net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'>> Accuracy of Model trained on Training set on training set: {(100 * correct / total):.2f}%')\n",
    "\n",
    "# test the accuracy of the model on train_dataloader\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images=images.to(device)\n",
    "        labels=labels.to(device)\n",
    "        outputs = simple_vision_net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'>> Accuracy of Model trained on Training set on testing set: {(100 * correct / total):.2f}%')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Simplified Vision Transformer Architecture=========\n",
      "to_patch_embedding.1.weight 3072\n",
      "to_patch_embedding.1.bias 3072\n",
      "to_patch_embedding.2.weight 3145728\n",
      "to_patch_embedding.2.bias 1024\n",
      "to_patch_embedding.3.weight 1024\n",
      "to_patch_embedding.3.bias 1024\n",
      "transformer.norm.weight 1024\n",
      "transformer.norm.bias 1024\n",
      "transformer.layers.0.0.norm.weight 1024\n",
      "transformer.layers.0.0.norm.bias 1024\n",
      "transformer.layers.0.0.to_qkv.weight 3145728\n",
      "transformer.layers.0.0.to_out.weight 1048576\n",
      "transformer.layers.0.1.net.0.weight 1024\n",
      "transformer.layers.0.1.net.0.bias 1024\n",
      "transformer.layers.0.1.net.1.weight 2097152\n",
      "transformer.layers.0.1.net.1.bias 2048\n",
      "transformer.layers.0.1.net.3.weight 2097152\n",
      "transformer.layers.0.1.net.3.bias 1024\n",
      "transformer.layers.1.0.norm.weight 1024\n",
      "transformer.layers.1.0.norm.bias 1024\n",
      "transformer.layers.1.0.to_qkv.weight 3145728\n",
      "transformer.layers.1.0.to_out.weight 1048576\n",
      "transformer.layers.1.1.net.0.weight 1024\n",
      "transformer.layers.1.1.net.0.bias 1024\n",
      "transformer.layers.1.1.net.1.weight 2097152\n",
      "transformer.layers.1.1.net.1.bias 2048\n",
      "transformer.layers.1.1.net.3.weight 2097152\n",
      "transformer.layers.1.1.net.3.bias 1024\n",
      "transformer.layers.2.0.norm.weight 1024\n",
      "transformer.layers.2.0.norm.bias 1024\n",
      "transformer.layers.2.0.to_qkv.weight 3145728\n",
      "transformer.layers.2.0.to_out.weight 1048576\n",
      "transformer.layers.2.1.net.0.weight 1024\n",
      "transformer.layers.2.1.net.0.bias 1024\n",
      "transformer.layers.2.1.net.1.weight 2097152\n",
      "transformer.layers.2.1.net.1.bias 2048\n",
      "transformer.layers.2.1.net.3.weight 2097152\n",
      "transformer.layers.2.1.net.3.bias 1024\n",
      "transformer.layers.3.0.norm.weight 1024\n",
      "transformer.layers.3.0.norm.bias 1024\n",
      "transformer.layers.3.0.to_qkv.weight 3145728\n",
      "transformer.layers.3.0.to_out.weight 1048576\n",
      "transformer.layers.3.1.net.0.weight 1024\n",
      "transformer.layers.3.1.net.0.bias 1024\n",
      "transformer.layers.3.1.net.1.weight 2097152\n",
      "transformer.layers.3.1.net.1.bias 2048\n",
      "transformer.layers.3.1.net.3.weight 2097152\n",
      "transformer.layers.3.1.net.3.bias 1024\n",
      "transformer.layers.4.0.norm.weight 1024\n",
      "transformer.layers.4.0.norm.bias 1024\n",
      "transformer.layers.4.0.to_qkv.weight 3145728\n",
      "transformer.layers.4.0.to_out.weight 1048576\n",
      "transformer.layers.4.1.net.0.weight 1024\n",
      "transformer.layers.4.1.net.0.bias 1024\n",
      "transformer.layers.4.1.net.1.weight 2097152\n",
      "transformer.layers.4.1.net.1.bias 2048\n",
      "transformer.layers.4.1.net.3.weight 2097152\n",
      "transformer.layers.4.1.net.3.bias 1024\n",
      "transformer.layers.5.0.norm.weight 1024\n",
      "transformer.layers.5.0.norm.bias 1024\n",
      "transformer.layers.5.0.to_qkv.weight 3145728\n",
      "transformer.layers.5.0.to_out.weight 1048576\n",
      "transformer.layers.5.1.net.0.weight 1024\n",
      "transformer.layers.5.1.net.0.bias 1024\n",
      "transformer.layers.5.1.net.1.weight 2097152\n",
      "transformer.layers.5.1.net.1.bias 2048\n",
      "transformer.layers.5.1.net.3.weight 2097152\n",
      "transformer.layers.5.1.net.3.bias 1024\n",
      "linear_head.weight 4096\n",
      "linear_head.bias 4\n"
     ]
    }
   ],
   "source": [
    "# count the number of parameters of vision_net\n",
    "print(\"========Simplified Vision Transformer Architecture=========\")\n",
    "for name, param in simple_vision_net.named_parameters():\n",
    "    print(name, param.numel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vision_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NaViT\n",
    "This paper proposes to leverage the flexibility of attention and masking for variable lengthed sequences to train images of multiple resolution, packed into a single batch. They demonstrate much faster training and improved accuracies, with the only cost being extra complexity in the architecture and dataloading. They use factorized 2d positional encodings, token dropping, as well as query-key normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-24 17:49:00] Epoch:  1 Loss of model:  1.3943312168121338\n",
      "[2023-08-24 17:54:23] Epoch:  20 Loss of model:  1.3738586902618408\n",
      "[2023-08-24 18:00:18] Epoch:  40 Loss of model:  1.3188676834106445\n",
      "[2023-08-24 18:06:09] Epoch:  60 Loss of model:  1.3631240129470825\n",
      "[2023-08-24 18:12:00] Epoch:  80 Loss of model:  1.3580167293548584\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vision_net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 98\u001b[0m\n\u001b[0;32m     96\u001b[0m     param\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[39m#---------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m vision_net\u001b[39m.\u001b[39meval()\n\u001b[0;32m    100\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m    101\u001b[0m     correct \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vision_net' is not defined"
     ]
    }
   ],
   "source": [
    "from vit_pytorch.na_vit import NaViT\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from torch.utils.data import TensorDataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.ndimage import rotate as scipyrotate\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from captum.attr import IntegratedGradients\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "epochs=80\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_data_dir = 'Training'\n",
    "\n",
    "mean = [0.4914, 0.4822, 0.4465]\n",
    "std = [0.2023, 0.1994, 0.2010]\n",
    "transform = transforms.Compose([\n",
    "                                transforms.Resize((224, 224)),  \n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                #  transforms.Normalize(mean=mean, std=std)\n",
    "                                 ])\n",
    "train_dataset = datasets.ImageFolder(root=train_data_dir, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_data_dir = 'Testing'\n",
    "test_dataset = datasets.ImageFolder(root=test_data_dir, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "sample_img=next(iter(train_loader))[0][0]\n",
    "im_size = (sample_img.shape[1], sample_img.shape[2])\n",
    "\n",
    "channel = 3\n",
    "\n",
    "num_classes =max(train_dataset.targets)+1\n",
    "\n",
    "\n",
    "\n",
    "na_vit = NaViT(\n",
    "    image_size = im_size[0],\n",
    "    patch_size = 32,\n",
    "    num_classes = num_classes,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1,\n",
    "    token_dropout_prob = 0.1  # token dropout of 10% (keep 90% of tokens)\n",
    ")\n",
    "\n",
    "\n",
    "def get_time():\n",
    "    return str(time.strftime(\"[%Y-%m-%d %H:%M:%S]\", time.localtime()))\n",
    "\n",
    "\n",
    "na_vit.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(na_vit.parameters(), lr=1e-3)\n",
    "\n",
    "# Teacher training before starting the dataset distillation process\n",
    "for epochy in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # convert data to list along first dimension\n",
    "        data = [list(data)]\n",
    "        output = na_vit(data) \n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epochy==0:\n",
    "        print(get_time(), 'Epoch: ', epochy+1 , 'Loss of model: ', loss.item())\n",
    "    elif (epochy+1)%20==0:\n",
    "        print(get_time(), 'Epoch: ', epochy+1 , 'Loss of model: ', loss.item())\n",
    "\n",
    "del loss\n",
    "del output\n",
    "\n",
    "\n",
    "# make all parameters non-trainable, so as to make image_syn the only trainable parameter\n",
    "for param in list(na_vit.parameters()):\n",
    "    param.requires_grad = False\n",
    "#---------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Accuracy of Model trained on Training set on training set: 28.64%\n",
      ">> Accuracy of Model trained on Training set on testing set: 29.19%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images=images.to(device)\n",
    "        labels=labels.to(device)\n",
    "        images = [list(images)]\n",
    "        outputs = na_vit(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'>> Accuracy of Model trained on Training set on training set: {(100 * correct / total):.2f}%')\n",
    "\n",
    "# test the accuracy of the model on train_dataloader\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images=images.to(device)\n",
    "        labels=labels.to(device)\n",
    "        images = [list(images)]\n",
    "        outputs = na_vit(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'>> Accuracy of Model trained on Training set on testing set: {(100 * correct / total):.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Na Vision Transformer Architecture=========\n",
      "pos_embed_height 7168\n",
      "pos_embed_width 7168\n",
      "attn_pool_queries 1024\n",
      "to_patch_embedding.0.gamma 3072\n",
      "to_patch_embedding.1.weight 3145728\n",
      "to_patch_embedding.1.bias 1024\n",
      "to_patch_embedding.2.gamma 1024\n",
      "transformer.layers.0.0.norm.gamma 1024\n",
      "transformer.layers.0.0.q_norm.gamma 1024\n",
      "transformer.layers.0.0.k_norm.gamma 1024\n",
      "transformer.layers.0.0.to_q.weight 1048576\n",
      "transformer.layers.0.0.to_kv.weight 2097152\n",
      "transformer.layers.0.0.to_out.0.weight 1048576\n",
      "transformer.layers.0.1.0.gamma 1024\n",
      "transformer.layers.0.1.1.weight 2097152\n",
      "transformer.layers.0.1.1.bias 2048\n",
      "transformer.layers.0.1.4.weight 2097152\n",
      "transformer.layers.0.1.4.bias 1024\n",
      "transformer.layers.1.0.norm.gamma 1024\n",
      "transformer.layers.1.0.q_norm.gamma 1024\n",
      "transformer.layers.1.0.k_norm.gamma 1024\n",
      "transformer.layers.1.0.to_q.weight 1048576\n",
      "transformer.layers.1.0.to_kv.weight 2097152\n",
      "transformer.layers.1.0.to_out.0.weight 1048576\n",
      "transformer.layers.1.1.0.gamma 1024\n",
      "transformer.layers.1.1.1.weight 2097152\n",
      "transformer.layers.1.1.1.bias 2048\n",
      "transformer.layers.1.1.4.weight 2097152\n",
      "transformer.layers.1.1.4.bias 1024\n",
      "transformer.layers.2.0.norm.gamma 1024\n",
      "transformer.layers.2.0.q_norm.gamma 1024\n",
      "transformer.layers.2.0.k_norm.gamma 1024\n",
      "transformer.layers.2.0.to_q.weight 1048576\n",
      "transformer.layers.2.0.to_kv.weight 2097152\n",
      "transformer.layers.2.0.to_out.0.weight 1048576\n",
      "transformer.layers.2.1.0.gamma 1024\n",
      "transformer.layers.2.1.1.weight 2097152\n",
      "transformer.layers.2.1.1.bias 2048\n",
      "transformer.layers.2.1.4.weight 2097152\n",
      "transformer.layers.2.1.4.bias 1024\n",
      "transformer.layers.3.0.norm.gamma 1024\n",
      "transformer.layers.3.0.q_norm.gamma 1024\n",
      "transformer.layers.3.0.k_norm.gamma 1024\n",
      "transformer.layers.3.0.to_q.weight 1048576\n",
      "transformer.layers.3.0.to_kv.weight 2097152\n",
      "transformer.layers.3.0.to_out.0.weight 1048576\n",
      "transformer.layers.3.1.0.gamma 1024\n",
      "transformer.layers.3.1.1.weight 2097152\n",
      "transformer.layers.3.1.1.bias 2048\n",
      "transformer.layers.3.1.4.weight 2097152\n",
      "transformer.layers.3.1.4.bias 1024\n",
      "transformer.layers.4.0.norm.gamma 1024\n",
      "transformer.layers.4.0.q_norm.gamma 1024\n",
      "transformer.layers.4.0.k_norm.gamma 1024\n",
      "transformer.layers.4.0.to_q.weight 1048576\n",
      "transformer.layers.4.0.to_kv.weight 2097152\n",
      "transformer.layers.4.0.to_out.0.weight 1048576\n",
      "transformer.layers.4.1.0.gamma 1024\n",
      "transformer.layers.4.1.1.weight 2097152\n",
      "transformer.layers.4.1.1.bias 2048\n",
      "transformer.layers.4.1.4.weight 2097152\n",
      "transformer.layers.4.1.4.bias 1024\n",
      "transformer.layers.5.0.norm.gamma 1024\n",
      "transformer.layers.5.0.q_norm.gamma 1024\n",
      "transformer.layers.5.0.k_norm.gamma 1024\n",
      "transformer.layers.5.0.to_q.weight 1048576\n",
      "transformer.layers.5.0.to_kv.weight 2097152\n",
      "transformer.layers.5.0.to_out.0.weight 1048576\n",
      "transformer.layers.5.1.0.gamma 1024\n",
      "transformer.layers.5.1.1.weight 2097152\n",
      "transformer.layers.5.1.1.bias 2048\n",
      "transformer.layers.5.1.4.weight 2097152\n",
      "transformer.layers.5.1.4.bias 1024\n",
      "transformer.norm.gamma 1024\n",
      "attn_pool.norm.gamma 1024\n",
      "attn_pool.q_norm.gamma 1024\n",
      "attn_pool.k_norm.gamma 1024\n",
      "attn_pool.to_q.weight 1048576\n",
      "attn_pool.to_kv.weight 2097152\n",
      "attn_pool.to_out.0.weight 1048576\n",
      "mlp_head.0.gamma 1024\n",
      "mlp_head.1.weight 4096\n"
     ]
    }
   ],
   "source": [
    "# count the number of parameters of vision_net\n",
    "print(\"========Na Vision Transformer Architecture=========\")\n",
    "for name, param in na_vit.named_parameters():\n",
    "    print(name, param.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained DeiT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\AMI_LAB/.cache\\torch\\hub\\facebookresearch_deit_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [2023-08-30 14:08:01] Epoch:  10 Loss of model:  0.007767439981841523\n",
      "\t [2023-08-30 14:16:11] Epoch:  20 Loss of model:  0.0014806994611559355\n",
      "\t [2023-08-30 14:24:23] Epoch:  30 Loss of model:  0.0006409694223016825\n"
     ]
    }
   ],
   "source": [
    "deit_model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n",
    "\n",
    "# Update the model's final classifier for 4 classes\n",
    "num_classes = 4\n",
    "deit_model.head = torch.nn.Linear(deit_model.head.in_features, num_classes)\n",
    "\n",
    "deit_model.to(device)\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=3),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=train_data_dir, transform=transform)\n",
    "trainloader= torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(deit_model.parameters(), lr=1e-5)\n",
    "\n",
    "# Teacher training before starting the dataset distillation process\n",
    "epochs=30\n",
    "for epochy in range(epochs):\n",
    "    running_loss=0.0\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = deit_model(data) \n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    if (epochy+1)%10==0:\n",
    "        print('\\t',get_time(), 'Epoch: ', epochy+1 , 'Loss of model: ', running_loss/len(trainloader))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 71.07%\n"
     ]
    }
   ],
   "source": [
    "deit_model.eval()\n",
    "for param in list(deit_model.parameters()):\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "# test the accuracy of the model on train_dataloader\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in valloader:\n",
    "        images=images.to(device)\n",
    "        labels=labels.to(device)\n",
    "        outputs = deit_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_acc=100 * correct / total\n",
    "    print(f'\\t>> Accuracy of Model trained on Training set on testing set: {val_acc:.2f}%')\n",
    "\n",
    "\n",
    "\n",
    "# del net\n",
    "del outputs\n",
    "del images\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Data Efficient Vision Transformer Architecture=========\n",
      "cls_token 768\n",
      "pos_embed 151296\n",
      "patch_embed.proj.weight 589824\n",
      "patch_embed.proj.bias 768\n",
      "blocks.0.norm1.weight 768\n",
      "blocks.0.norm1.bias 768\n",
      "blocks.0.attn.qkv.weight 1769472\n",
      "blocks.0.attn.qkv.bias 2304\n",
      "blocks.0.attn.proj.weight 589824\n",
      "blocks.0.attn.proj.bias 768\n",
      "blocks.0.norm2.weight 768\n",
      "blocks.0.norm2.bias 768\n",
      "blocks.0.mlp.fc1.weight 2359296\n",
      "blocks.0.mlp.fc1.bias 3072\n",
      "blocks.0.mlp.fc2.weight 2359296\n",
      "blocks.0.mlp.fc2.bias 768\n",
      "blocks.1.norm1.weight 768\n",
      "blocks.1.norm1.bias 768\n",
      "blocks.1.attn.qkv.weight 1769472\n",
      "blocks.1.attn.qkv.bias 2304\n",
      "blocks.1.attn.proj.weight 589824\n",
      "blocks.1.attn.proj.bias 768\n",
      "blocks.1.norm2.weight 768\n",
      "blocks.1.norm2.bias 768\n",
      "blocks.1.mlp.fc1.weight 2359296\n",
      "blocks.1.mlp.fc1.bias 3072\n",
      "blocks.1.mlp.fc2.weight 2359296\n",
      "blocks.1.mlp.fc2.bias 768\n",
      "blocks.2.norm1.weight 768\n",
      "blocks.2.norm1.bias 768\n",
      "blocks.2.attn.qkv.weight 1769472\n",
      "blocks.2.attn.qkv.bias 2304\n",
      "blocks.2.attn.proj.weight 589824\n",
      "blocks.2.attn.proj.bias 768\n",
      "blocks.2.norm2.weight 768\n",
      "blocks.2.norm2.bias 768\n",
      "blocks.2.mlp.fc1.weight 2359296\n",
      "blocks.2.mlp.fc1.bias 3072\n",
      "blocks.2.mlp.fc2.weight 2359296\n",
      "blocks.2.mlp.fc2.bias 768\n",
      "blocks.3.norm1.weight 768\n",
      "blocks.3.norm1.bias 768\n",
      "blocks.3.attn.qkv.weight 1769472\n",
      "blocks.3.attn.qkv.bias 2304\n",
      "blocks.3.attn.proj.weight 589824\n",
      "blocks.3.attn.proj.bias 768\n",
      "blocks.3.norm2.weight 768\n",
      "blocks.3.norm2.bias 768\n",
      "blocks.3.mlp.fc1.weight 2359296\n",
      "blocks.3.mlp.fc1.bias 3072\n",
      "blocks.3.mlp.fc2.weight 2359296\n",
      "blocks.3.mlp.fc2.bias 768\n",
      "blocks.4.norm1.weight 768\n",
      "blocks.4.norm1.bias 768\n",
      "blocks.4.attn.qkv.weight 1769472\n",
      "blocks.4.attn.qkv.bias 2304\n",
      "blocks.4.attn.proj.weight 589824\n",
      "blocks.4.attn.proj.bias 768\n",
      "blocks.4.norm2.weight 768\n",
      "blocks.4.norm2.bias 768\n",
      "blocks.4.mlp.fc1.weight 2359296\n",
      "blocks.4.mlp.fc1.bias 3072\n",
      "blocks.4.mlp.fc2.weight 2359296\n",
      "blocks.4.mlp.fc2.bias 768\n",
      "blocks.5.norm1.weight 768\n",
      "blocks.5.norm1.bias 768\n",
      "blocks.5.attn.qkv.weight 1769472\n",
      "blocks.5.attn.qkv.bias 2304\n",
      "blocks.5.attn.proj.weight 589824\n",
      "blocks.5.attn.proj.bias 768\n",
      "blocks.5.norm2.weight 768\n",
      "blocks.5.norm2.bias 768\n",
      "blocks.5.mlp.fc1.weight 2359296\n",
      "blocks.5.mlp.fc1.bias 3072\n",
      "blocks.5.mlp.fc2.weight 2359296\n",
      "blocks.5.mlp.fc2.bias 768\n",
      "blocks.6.norm1.weight 768\n",
      "blocks.6.norm1.bias 768\n",
      "blocks.6.attn.qkv.weight 1769472\n",
      "blocks.6.attn.qkv.bias 2304\n",
      "blocks.6.attn.proj.weight 589824\n",
      "blocks.6.attn.proj.bias 768\n",
      "blocks.6.norm2.weight 768\n",
      "blocks.6.norm2.bias 768\n",
      "blocks.6.mlp.fc1.weight 2359296\n",
      "blocks.6.mlp.fc1.bias 3072\n",
      "blocks.6.mlp.fc2.weight 2359296\n",
      "blocks.6.mlp.fc2.bias 768\n",
      "blocks.7.norm1.weight 768\n",
      "blocks.7.norm1.bias 768\n",
      "blocks.7.attn.qkv.weight 1769472\n",
      "blocks.7.attn.qkv.bias 2304\n",
      "blocks.7.attn.proj.weight 589824\n",
      "blocks.7.attn.proj.bias 768\n",
      "blocks.7.norm2.weight 768\n",
      "blocks.7.norm2.bias 768\n",
      "blocks.7.mlp.fc1.weight 2359296\n",
      "blocks.7.mlp.fc1.bias 3072\n",
      "blocks.7.mlp.fc2.weight 2359296\n",
      "blocks.7.mlp.fc2.bias 768\n",
      "blocks.8.norm1.weight 768\n",
      "blocks.8.norm1.bias 768\n",
      "blocks.8.attn.qkv.weight 1769472\n",
      "blocks.8.attn.qkv.bias 2304\n",
      "blocks.8.attn.proj.weight 589824\n",
      "blocks.8.attn.proj.bias 768\n",
      "blocks.8.norm2.weight 768\n",
      "blocks.8.norm2.bias 768\n",
      "blocks.8.mlp.fc1.weight 2359296\n",
      "blocks.8.mlp.fc1.bias 3072\n",
      "blocks.8.mlp.fc2.weight 2359296\n",
      "blocks.8.mlp.fc2.bias 768\n",
      "blocks.9.norm1.weight 768\n",
      "blocks.9.norm1.bias 768\n",
      "blocks.9.attn.qkv.weight 1769472\n",
      "blocks.9.attn.qkv.bias 2304\n",
      "blocks.9.attn.proj.weight 589824\n",
      "blocks.9.attn.proj.bias 768\n",
      "blocks.9.norm2.weight 768\n",
      "blocks.9.norm2.bias 768\n",
      "blocks.9.mlp.fc1.weight 2359296\n",
      "blocks.9.mlp.fc1.bias 3072\n",
      "blocks.9.mlp.fc2.weight 2359296\n",
      "blocks.9.mlp.fc2.bias 768\n",
      "blocks.10.norm1.weight 768\n",
      "blocks.10.norm1.bias 768\n",
      "blocks.10.attn.qkv.weight 1769472\n",
      "blocks.10.attn.qkv.bias 2304\n",
      "blocks.10.attn.proj.weight 589824\n",
      "blocks.10.attn.proj.bias 768\n",
      "blocks.10.norm2.weight 768\n",
      "blocks.10.norm2.bias 768\n",
      "blocks.10.mlp.fc1.weight 2359296\n",
      "blocks.10.mlp.fc1.bias 3072\n",
      "blocks.10.mlp.fc2.weight 2359296\n",
      "blocks.10.mlp.fc2.bias 768\n",
      "blocks.11.norm1.weight 768\n",
      "blocks.11.norm1.bias 768\n",
      "blocks.11.attn.qkv.weight 1769472\n",
      "blocks.11.attn.qkv.bias 2304\n",
      "blocks.11.attn.proj.weight 589824\n",
      "blocks.11.attn.proj.bias 768\n",
      "blocks.11.norm2.weight 768\n",
      "blocks.11.norm2.bias 768\n",
      "blocks.11.mlp.fc1.weight 2359296\n",
      "blocks.11.mlp.fc1.bias 3072\n",
      "blocks.11.mlp.fc2.weight 2359296\n",
      "blocks.11.mlp.fc2.bias 768\n",
      "norm.weight 768\n",
      "norm.bias 768\n",
      "head.weight 3072\n",
      "head.bias 4\n"
     ]
    }
   ],
   "source": [
    "# count the number of parameters of vision_net\n",
    "print(\"========Data Efficient Vision Transformer Architecture=========\")\n",
    "for name, param in deit_model.named_parameters():\n",
    "    print(name, param.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth\" to C:\\Users\\AMI_LAB/.cache\\torch\\hub\\checkpoints\\efficientnet-b0-355c32eb.pth\n",
      "100%|██████████| 20.4M/20.4M [00:02<00:00, 8.50MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "model = EfficientNet.from_pretrained('efficientnet-b0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Data Efficient Vision Transformer Architecture=========\n",
      "_conv_stem.weight 864\n",
      "_bn0.weight 32\n",
      "_bn0.bias 32\n",
      "_blocks.0._depthwise_conv.weight 288\n",
      "_blocks.0._bn1.weight 32\n",
      "_blocks.0._bn1.bias 32\n",
      "_blocks.0._se_reduce.weight 256\n",
      "_blocks.0._se_reduce.bias 8\n",
      "_blocks.0._se_expand.weight 256\n",
      "_blocks.0._se_expand.bias 32\n",
      "_blocks.0._project_conv.weight 512\n",
      "_blocks.0._bn2.weight 16\n",
      "_blocks.0._bn2.bias 16\n",
      "_blocks.1._expand_conv.weight 1536\n",
      "_blocks.1._bn0.weight 96\n",
      "_blocks.1._bn0.bias 96\n",
      "_blocks.1._depthwise_conv.weight 864\n",
      "_blocks.1._bn1.weight 96\n",
      "_blocks.1._bn1.bias 96\n",
      "_blocks.1._se_reduce.weight 384\n",
      "_blocks.1._se_reduce.bias 4\n",
      "_blocks.1._se_expand.weight 384\n",
      "_blocks.1._se_expand.bias 96\n",
      "_blocks.1._project_conv.weight 2304\n",
      "_blocks.1._bn2.weight 24\n",
      "_blocks.1._bn2.bias 24\n",
      "_blocks.2._expand_conv.weight 3456\n",
      "_blocks.2._bn0.weight 144\n",
      "_blocks.2._bn0.bias 144\n",
      "_blocks.2._depthwise_conv.weight 1296\n",
      "_blocks.2._bn1.weight 144\n",
      "_blocks.2._bn1.bias 144\n",
      "_blocks.2._se_reduce.weight 864\n",
      "_blocks.2._se_reduce.bias 6\n",
      "_blocks.2._se_expand.weight 864\n",
      "_blocks.2._se_expand.bias 144\n",
      "_blocks.2._project_conv.weight 3456\n",
      "_blocks.2._bn2.weight 24\n",
      "_blocks.2._bn2.bias 24\n",
      "_blocks.3._expand_conv.weight 3456\n",
      "_blocks.3._bn0.weight 144\n",
      "_blocks.3._bn0.bias 144\n",
      "_blocks.3._depthwise_conv.weight 3600\n",
      "_blocks.3._bn1.weight 144\n",
      "_blocks.3._bn1.bias 144\n",
      "_blocks.3._se_reduce.weight 864\n",
      "_blocks.3._se_reduce.bias 6\n",
      "_blocks.3._se_expand.weight 864\n",
      "_blocks.3._se_expand.bias 144\n",
      "_blocks.3._project_conv.weight 5760\n",
      "_blocks.3._bn2.weight 40\n",
      "_blocks.3._bn2.bias 40\n",
      "_blocks.4._expand_conv.weight 9600\n",
      "_blocks.4._bn0.weight 240\n",
      "_blocks.4._bn0.bias 240\n",
      "_blocks.4._depthwise_conv.weight 6000\n",
      "_blocks.4._bn1.weight 240\n",
      "_blocks.4._bn1.bias 240\n",
      "_blocks.4._se_reduce.weight 2400\n",
      "_blocks.4._se_reduce.bias 10\n",
      "_blocks.4._se_expand.weight 2400\n",
      "_blocks.4._se_expand.bias 240\n",
      "_blocks.4._project_conv.weight 9600\n",
      "_blocks.4._bn2.weight 40\n",
      "_blocks.4._bn2.bias 40\n",
      "_blocks.5._expand_conv.weight 9600\n",
      "_blocks.5._bn0.weight 240\n",
      "_blocks.5._bn0.bias 240\n",
      "_blocks.5._depthwise_conv.weight 2160\n",
      "_blocks.5._bn1.weight 240\n",
      "_blocks.5._bn1.bias 240\n",
      "_blocks.5._se_reduce.weight 2400\n",
      "_blocks.5._se_reduce.bias 10\n",
      "_blocks.5._se_expand.weight 2400\n",
      "_blocks.5._se_expand.bias 240\n",
      "_blocks.5._project_conv.weight 19200\n",
      "_blocks.5._bn2.weight 80\n",
      "_blocks.5._bn2.bias 80\n",
      "_blocks.6._expand_conv.weight 38400\n",
      "_blocks.6._bn0.weight 480\n",
      "_blocks.6._bn0.bias 480\n",
      "_blocks.6._depthwise_conv.weight 4320\n",
      "_blocks.6._bn1.weight 480\n",
      "_blocks.6._bn1.bias 480\n",
      "_blocks.6._se_reduce.weight 9600\n",
      "_blocks.6._se_reduce.bias 20\n",
      "_blocks.6._se_expand.weight 9600\n",
      "_blocks.6._se_expand.bias 480\n",
      "_blocks.6._project_conv.weight 38400\n",
      "_blocks.6._bn2.weight 80\n",
      "_blocks.6._bn2.bias 80\n",
      "_blocks.7._expand_conv.weight 38400\n",
      "_blocks.7._bn0.weight 480\n",
      "_blocks.7._bn0.bias 480\n",
      "_blocks.7._depthwise_conv.weight 4320\n",
      "_blocks.7._bn1.weight 480\n",
      "_blocks.7._bn1.bias 480\n",
      "_blocks.7._se_reduce.weight 9600\n",
      "_blocks.7._se_reduce.bias 20\n",
      "_blocks.7._se_expand.weight 9600\n",
      "_blocks.7._se_expand.bias 480\n",
      "_blocks.7._project_conv.weight 38400\n",
      "_blocks.7._bn2.weight 80\n",
      "_blocks.7._bn2.bias 80\n",
      "_blocks.8._expand_conv.weight 38400\n",
      "_blocks.8._bn0.weight 480\n",
      "_blocks.8._bn0.bias 480\n",
      "_blocks.8._depthwise_conv.weight 12000\n",
      "_blocks.8._bn1.weight 480\n",
      "_blocks.8._bn1.bias 480\n",
      "_blocks.8._se_reduce.weight 9600\n",
      "_blocks.8._se_reduce.bias 20\n",
      "_blocks.8._se_expand.weight 9600\n",
      "_blocks.8._se_expand.bias 480\n",
      "_blocks.8._project_conv.weight 53760\n",
      "_blocks.8._bn2.weight 112\n",
      "_blocks.8._bn2.bias 112\n",
      "_blocks.9._expand_conv.weight 75264\n",
      "_blocks.9._bn0.weight 672\n",
      "_blocks.9._bn0.bias 672\n",
      "_blocks.9._depthwise_conv.weight 16800\n",
      "_blocks.9._bn1.weight 672\n",
      "_blocks.9._bn1.bias 672\n",
      "_blocks.9._se_reduce.weight 18816\n",
      "_blocks.9._se_reduce.bias 28\n",
      "_blocks.9._se_expand.weight 18816\n",
      "_blocks.9._se_expand.bias 672\n",
      "_blocks.9._project_conv.weight 75264\n",
      "_blocks.9._bn2.weight 112\n",
      "_blocks.9._bn2.bias 112\n",
      "_blocks.10._expand_conv.weight 75264\n",
      "_blocks.10._bn0.weight 672\n",
      "_blocks.10._bn0.bias 672\n",
      "_blocks.10._depthwise_conv.weight 16800\n",
      "_blocks.10._bn1.weight 672\n",
      "_blocks.10._bn1.bias 672\n",
      "_blocks.10._se_reduce.weight 18816\n",
      "_blocks.10._se_reduce.bias 28\n",
      "_blocks.10._se_expand.weight 18816\n",
      "_blocks.10._se_expand.bias 672\n",
      "_blocks.10._project_conv.weight 75264\n",
      "_blocks.10._bn2.weight 112\n",
      "_blocks.10._bn2.bias 112\n",
      "_blocks.11._expand_conv.weight 75264\n",
      "_blocks.11._bn0.weight 672\n",
      "_blocks.11._bn0.bias 672\n",
      "_blocks.11._depthwise_conv.weight 16800\n",
      "_blocks.11._bn1.weight 672\n",
      "_blocks.11._bn1.bias 672\n",
      "_blocks.11._se_reduce.weight 18816\n",
      "_blocks.11._se_reduce.bias 28\n",
      "_blocks.11._se_expand.weight 18816\n",
      "_blocks.11._se_expand.bias 672\n",
      "_blocks.11._project_conv.weight 129024\n",
      "_blocks.11._bn2.weight 192\n",
      "_blocks.11._bn2.bias 192\n",
      "_blocks.12._expand_conv.weight 221184\n",
      "_blocks.12._bn0.weight 1152\n",
      "_blocks.12._bn0.bias 1152\n",
      "_blocks.12._depthwise_conv.weight 28800\n",
      "_blocks.12._bn1.weight 1152\n",
      "_blocks.12._bn1.bias 1152\n",
      "_blocks.12._se_reduce.weight 55296\n",
      "_blocks.12._se_reduce.bias 48\n",
      "_blocks.12._se_expand.weight 55296\n",
      "_blocks.12._se_expand.bias 1152\n",
      "_blocks.12._project_conv.weight 221184\n",
      "_blocks.12._bn2.weight 192\n",
      "_blocks.12._bn2.bias 192\n",
      "_blocks.13._expand_conv.weight 221184\n",
      "_blocks.13._bn0.weight 1152\n",
      "_blocks.13._bn0.bias 1152\n",
      "_blocks.13._depthwise_conv.weight 28800\n",
      "_blocks.13._bn1.weight 1152\n",
      "_blocks.13._bn1.bias 1152\n",
      "_blocks.13._se_reduce.weight 55296\n",
      "_blocks.13._se_reduce.bias 48\n",
      "_blocks.13._se_expand.weight 55296\n",
      "_blocks.13._se_expand.bias 1152\n",
      "_blocks.13._project_conv.weight 221184\n",
      "_blocks.13._bn2.weight 192\n",
      "_blocks.13._bn2.bias 192\n",
      "_blocks.14._expand_conv.weight 221184\n",
      "_blocks.14._bn0.weight 1152\n",
      "_blocks.14._bn0.bias 1152\n",
      "_blocks.14._depthwise_conv.weight 28800\n",
      "_blocks.14._bn1.weight 1152\n",
      "_blocks.14._bn1.bias 1152\n",
      "_blocks.14._se_reduce.weight 55296\n",
      "_blocks.14._se_reduce.bias 48\n",
      "_blocks.14._se_expand.weight 55296\n",
      "_blocks.14._se_expand.bias 1152\n",
      "_blocks.14._project_conv.weight 221184\n",
      "_blocks.14._bn2.weight 192\n",
      "_blocks.14._bn2.bias 192\n",
      "_blocks.15._expand_conv.weight 221184\n",
      "_blocks.15._bn0.weight 1152\n",
      "_blocks.15._bn0.bias 1152\n",
      "_blocks.15._depthwise_conv.weight 10368\n",
      "_blocks.15._bn1.weight 1152\n",
      "_blocks.15._bn1.bias 1152\n",
      "_blocks.15._se_reduce.weight 55296\n",
      "_blocks.15._se_reduce.bias 48\n",
      "_blocks.15._se_expand.weight 55296\n",
      "_blocks.15._se_expand.bias 1152\n",
      "_blocks.15._project_conv.weight 368640\n",
      "_blocks.15._bn2.weight 320\n",
      "_blocks.15._bn2.bias 320\n",
      "_conv_head.weight 409600\n",
      "_bn1.weight 1280\n",
      "_bn1.bias 1280\n",
      "_fc.weight 1280000\n",
      "_fc.bias 1000\n"
     ]
    }
   ],
   "source": [
    "# count the number of parameters of vision_net\n",
    "print(\"========Data Efficient Vision Transformer Architecture=========\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [2023-08-30 15:10:16] Epoch:  10 Loss of model:  0.11445411359486372\n",
      "\t [2023-08-30 15:14:24] Epoch:  20 Loss of model:  0.04076709282463011\n",
      "\t [2023-08-30 15:18:31] Epoch:  30 Loss of model:  0.02178373228272666\n",
      "\t [2023-08-30 15:22:38] Epoch:  40 Loss of model:  0.010593964951112866\n",
      "\t [2023-08-30 15:26:46] Epoch:  50 Loss of model:  0.00936040855721449\n",
      "\t [2023-08-30 15:30:53] Epoch:  60 Loss of model:  0.010032777391050173\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Update the model's final classifier for 4 classes\n",
    "num_classes = 4\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "class ModifiedModel(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(ModifiedModel, self).__init__()\n",
    "        # Create a new classifier head with output size 4\n",
    "        self.featurizer= original_model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1000, 128),  # Adjust input size accordingly\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 4)  # Output size of 4 for 4 classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use the modified classifier for forward pass\n",
    "        x= self.featurizer(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "efficient_net_model = ModifiedModel(model).to(device)\n",
    "\n",
    "deit_model.to(device)\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=3),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=train_data_dir, transform=transform)\n",
    "trainloader= torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(efficient_net_model.parameters(), lr=1e-5)\n",
    "\n",
    "# Teacher training before starting the dataset distillation process\n",
    "epochs=60\n",
    "for epochy in range(epochs):\n",
    "    running_loss=0.0\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = efficient_net_model (data) \n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    if (epochy+1)%10==0:\n",
    "        print('\\t',get_time(), 'Epoch: ', epochy+1 , 'Loss of model: ', running_loss/len(trainloader))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>> Accuracy of Model trained on Training set on testing set: 67.26%\n"
     ]
    }
   ],
   "source": [
    "deit_model.eval()\n",
    "for param in list(efficient_net_model.parameters()):\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "# test the accuracy of the model on train_dataloader\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in valloader:\n",
    "        images=images.to(device)\n",
    "        labels=labels.to(device)\n",
    "        outputs = efficient_net_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_acc=100 * correct / total\n",
    "    print(f'\\t>> Accuracy of Model trained on Training set on testing set: {val_acc:.2f}%')\n",
    "\n",
    "\n",
    "\n",
    "# del net\n",
    "del outputs\n",
    "del images\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [2023-08-30 16:24:21] Epoch:  10 Loss of model:  0.6544434583705404\n",
      "\t [2023-08-30 16:29:57] Epoch:  20 Loss of model:  0.08491142482861229\n",
      "\t [2023-08-30 16:35:33] Epoch:  30 Loss of model:  0.0354600147060726\n",
      "\t>> Accuracy of Model trained on Training set on testing set: 37.56%\n",
      "========Data Efficient Vision Transformer Architecture=========\n",
      "features.0.0.weight 648\n",
      "features.0.1.weight 24\n",
      "features.0.1.bias 24\n",
      "features.1.conv.0.weight 5184\n",
      "features.1.conv.1.weight 24\n",
      "features.1.conv.1.bias 24\n",
      "features.1.conv.3.weight 576\n",
      "features.1.conv.4.weight 24\n",
      "features.1.conv.4.bias 24\n",
      "features.2.conv.0.weight 5184\n",
      "features.2.conv.1.weight 24\n",
      "features.2.conv.1.bias 24\n",
      "features.2.conv.3.weight 576\n",
      "features.2.conv.4.weight 24\n",
      "features.2.conv.4.bias 24\n",
      "features.3.conv.0.weight 20736\n",
      "features.3.conv.1.weight 96\n",
      "features.3.conv.1.bias 96\n",
      "features.3.conv.3.weight 4608\n",
      "features.3.conv.4.weight 48\n",
      "features.3.conv.4.bias 48\n",
      "features.4.conv.0.weight 82944\n",
      "features.4.conv.1.weight 192\n",
      "features.4.conv.1.bias 192\n",
      "features.4.conv.3.weight 9216\n",
      "features.4.conv.4.weight 48\n",
      "features.4.conv.4.bias 48\n",
      "features.5.conv.0.weight 82944\n",
      "features.5.conv.1.weight 192\n",
      "features.5.conv.1.bias 192\n",
      "features.5.conv.3.weight 9216\n",
      "features.5.conv.4.weight 48\n",
      "features.5.conv.4.bias 48\n",
      "features.6.conv.0.weight 82944\n",
      "features.6.conv.1.weight 192\n",
      "features.6.conv.1.bias 192\n",
      "features.6.conv.3.weight 9216\n",
      "features.6.conv.4.weight 48\n",
      "features.6.conv.4.bias 48\n",
      "features.7.conv.0.weight 82944\n",
      "features.7.conv.1.weight 192\n",
      "features.7.conv.1.bias 192\n",
      "features.7.conv.3.weight 12288\n",
      "features.7.conv.4.weight 64\n",
      "features.7.conv.4.bias 64\n",
      "features.8.conv.0.weight 147456\n",
      "features.8.conv.1.weight 256\n",
      "features.8.conv.1.bias 256\n",
      "features.8.conv.3.weight 16384\n",
      "features.8.conv.4.weight 64\n",
      "features.8.conv.4.bias 64\n",
      "features.9.conv.0.weight 147456\n",
      "features.9.conv.1.weight 256\n",
      "features.9.conv.1.bias 256\n",
      "features.9.conv.3.weight 16384\n",
      "features.9.conv.4.weight 64\n",
      "features.9.conv.4.bias 64\n",
      "features.10.conv.0.weight 147456\n",
      "features.10.conv.1.weight 256\n",
      "features.10.conv.1.bias 256\n",
      "features.10.conv.3.weight 16384\n",
      "features.10.conv.4.weight 64\n",
      "features.10.conv.4.bias 64\n",
      "features.11.conv.0.weight 16384\n",
      "features.11.conv.1.weight 256\n",
      "features.11.conv.1.bias 256\n",
      "features.11.conv.3.weight 2304\n",
      "features.11.conv.4.weight 256\n",
      "features.11.conv.4.bias 256\n",
      "features.11.conv.6.fc.0.weight 4096\n",
      "features.11.conv.6.fc.0.bias 16\n",
      "features.11.conv.6.fc.2.weight 4096\n",
      "features.11.conv.6.fc.2.bias 256\n",
      "features.11.conv.7.weight 32768\n",
      "features.11.conv.8.weight 128\n",
      "features.11.conv.8.bias 128\n",
      "features.12.conv.0.weight 65536\n",
      "features.12.conv.1.weight 512\n",
      "features.12.conv.1.bias 512\n",
      "features.12.conv.3.weight 4608\n",
      "features.12.conv.4.weight 512\n",
      "features.12.conv.4.bias 512\n",
      "features.12.conv.6.fc.0.weight 16384\n",
      "features.12.conv.6.fc.0.bias 32\n",
      "features.12.conv.6.fc.2.weight 16384\n",
      "features.12.conv.6.fc.2.bias 512\n",
      "features.12.conv.7.weight 65536\n",
      "features.12.conv.8.weight 128\n",
      "features.12.conv.8.bias 128\n",
      "features.13.conv.0.weight 65536\n",
      "features.13.conv.1.weight 512\n",
      "features.13.conv.1.bias 512\n",
      "features.13.conv.3.weight 4608\n",
      "features.13.conv.4.weight 512\n",
      "features.13.conv.4.bias 512\n",
      "features.13.conv.6.fc.0.weight 16384\n",
      "features.13.conv.6.fc.0.bias 32\n",
      "features.13.conv.6.fc.2.weight 16384\n",
      "features.13.conv.6.fc.2.bias 512\n",
      "features.13.conv.7.weight 65536\n",
      "features.13.conv.8.weight 128\n",
      "features.13.conv.8.bias 128\n",
      "features.14.conv.0.weight 65536\n",
      "features.14.conv.1.weight 512\n",
      "features.14.conv.1.bias 512\n",
      "features.14.conv.3.weight 4608\n",
      "features.14.conv.4.weight 512\n",
      "features.14.conv.4.bias 512\n",
      "features.14.conv.6.fc.0.weight 16384\n",
      "features.14.conv.6.fc.0.bias 32\n",
      "features.14.conv.6.fc.2.weight 16384\n",
      "features.14.conv.6.fc.2.bias 512\n",
      "features.14.conv.7.weight 65536\n",
      "features.14.conv.8.weight 128\n",
      "features.14.conv.8.bias 128\n",
      "features.15.conv.0.weight 65536\n",
      "features.15.conv.1.weight 512\n",
      "features.15.conv.1.bias 512\n",
      "features.15.conv.3.weight 4608\n",
      "features.15.conv.4.weight 512\n",
      "features.15.conv.4.bias 512\n",
      "features.15.conv.6.fc.0.weight 16384\n",
      "features.15.conv.6.fc.0.bias 32\n",
      "features.15.conv.6.fc.2.weight 16384\n",
      "features.15.conv.6.fc.2.bias 512\n",
      "features.15.conv.7.weight 65536\n",
      "features.15.conv.8.weight 128\n",
      "features.15.conv.8.bias 128\n",
      "features.16.conv.0.weight 65536\n",
      "features.16.conv.1.weight 512\n",
      "features.16.conv.1.bias 512\n",
      "features.16.conv.3.weight 4608\n",
      "features.16.conv.4.weight 512\n",
      "features.16.conv.4.bias 512\n",
      "features.16.conv.6.fc.0.weight 16384\n",
      "features.16.conv.6.fc.0.bias 32\n",
      "features.16.conv.6.fc.2.weight 16384\n",
      "features.16.conv.6.fc.2.bias 512\n",
      "features.16.conv.7.weight 65536\n",
      "features.16.conv.8.weight 128\n",
      "features.16.conv.8.bias 128\n",
      "features.17.conv.0.weight 98304\n",
      "features.17.conv.1.weight 768\n",
      "features.17.conv.1.bias 768\n",
      "features.17.conv.3.weight 6912\n",
      "features.17.conv.4.weight 768\n",
      "features.17.conv.4.bias 768\n",
      "features.17.conv.6.fc.0.weight 24576\n",
      "features.17.conv.6.fc.0.bias 32\n",
      "features.17.conv.6.fc.2.weight 24576\n",
      "features.17.conv.6.fc.2.bias 768\n",
      "features.17.conv.7.weight 122880\n",
      "features.17.conv.8.weight 160\n",
      "features.17.conv.8.bias 160\n",
      "features.18.conv.0.weight 153600\n",
      "features.18.conv.1.weight 960\n",
      "features.18.conv.1.bias 960\n",
      "features.18.conv.3.weight 8640\n",
      "features.18.conv.4.weight 960\n",
      "features.18.conv.4.bias 960\n",
      "features.18.conv.6.fc.0.weight 38400\n",
      "features.18.conv.6.fc.0.bias 40\n",
      "features.18.conv.6.fc.2.weight 38400\n",
      "features.18.conv.6.fc.2.bias 960\n",
      "features.18.conv.7.weight 153600\n",
      "features.18.conv.8.weight 160\n",
      "features.18.conv.8.bias 160\n",
      "features.19.conv.0.weight 153600\n",
      "features.19.conv.1.weight 960\n",
      "features.19.conv.1.bias 960\n",
      "features.19.conv.3.weight 8640\n",
      "features.19.conv.4.weight 960\n",
      "features.19.conv.4.bias 960\n",
      "features.19.conv.6.fc.0.weight 38400\n",
      "features.19.conv.6.fc.0.bias 40\n",
      "features.19.conv.6.fc.2.weight 38400\n",
      "features.19.conv.6.fc.2.bias 960\n",
      "features.19.conv.7.weight 153600\n",
      "features.19.conv.8.weight 160\n",
      "features.19.conv.8.bias 160\n",
      "features.20.conv.0.weight 153600\n",
      "features.20.conv.1.weight 960\n",
      "features.20.conv.1.bias 960\n",
      "features.20.conv.3.weight 8640\n",
      "features.20.conv.4.weight 960\n",
      "features.20.conv.4.bias 960\n",
      "features.20.conv.6.fc.0.weight 38400\n",
      "features.20.conv.6.fc.0.bias 40\n",
      "features.20.conv.6.fc.2.weight 38400\n",
      "features.20.conv.6.fc.2.bias 960\n",
      "features.20.conv.7.weight 153600\n",
      "features.20.conv.8.weight 160\n",
      "features.20.conv.8.bias 160\n",
      "features.21.conv.0.weight 153600\n",
      "features.21.conv.1.weight 960\n",
      "features.21.conv.1.bias 960\n",
      "features.21.conv.3.weight 8640\n",
      "features.21.conv.4.weight 960\n",
      "features.21.conv.4.bias 960\n",
      "features.21.conv.6.fc.0.weight 38400\n",
      "features.21.conv.6.fc.0.bias 40\n",
      "features.21.conv.6.fc.2.weight 38400\n",
      "features.21.conv.6.fc.2.bias 960\n",
      "features.21.conv.7.weight 153600\n",
      "features.21.conv.8.weight 160\n",
      "features.21.conv.8.bias 160\n",
      "features.22.conv.0.weight 153600\n",
      "features.22.conv.1.weight 960\n",
      "features.22.conv.1.bias 960\n",
      "features.22.conv.3.weight 8640\n",
      "features.22.conv.4.weight 960\n",
      "features.22.conv.4.bias 960\n",
      "features.22.conv.6.fc.0.weight 38400\n",
      "features.22.conv.6.fc.0.bias 40\n",
      "features.22.conv.6.fc.2.weight 38400\n",
      "features.22.conv.6.fc.2.bias 960\n",
      "features.22.conv.7.weight 153600\n",
      "features.22.conv.8.weight 160\n",
      "features.22.conv.8.bias 160\n",
      "features.23.conv.0.weight 153600\n",
      "features.23.conv.1.weight 960\n",
      "features.23.conv.1.bias 960\n",
      "features.23.conv.3.weight 8640\n",
      "features.23.conv.4.weight 960\n",
      "features.23.conv.4.bias 960\n",
      "features.23.conv.6.fc.0.weight 38400\n",
      "features.23.conv.6.fc.0.bias 40\n",
      "features.23.conv.6.fc.2.weight 38400\n",
      "features.23.conv.6.fc.2.bias 960\n",
      "features.23.conv.7.weight 153600\n",
      "features.23.conv.8.weight 160\n",
      "features.23.conv.8.bias 160\n",
      "features.24.conv.0.weight 153600\n",
      "features.24.conv.1.weight 960\n",
      "features.24.conv.1.bias 960\n",
      "features.24.conv.3.weight 8640\n",
      "features.24.conv.4.weight 960\n",
      "features.24.conv.4.bias 960\n",
      "features.24.conv.6.fc.0.weight 38400\n",
      "features.24.conv.6.fc.0.bias 40\n",
      "features.24.conv.6.fc.2.weight 38400\n",
      "features.24.conv.6.fc.2.bias 960\n",
      "features.24.conv.7.weight 153600\n",
      "features.24.conv.8.weight 160\n",
      "features.24.conv.8.bias 160\n",
      "features.25.conv.0.weight 153600\n",
      "features.25.conv.1.weight 960\n",
      "features.25.conv.1.bias 960\n",
      "features.25.conv.3.weight 8640\n",
      "features.25.conv.4.weight 960\n",
      "features.25.conv.4.bias 960\n",
      "features.25.conv.6.fc.0.weight 38400\n",
      "features.25.conv.6.fc.0.bias 40\n",
      "features.25.conv.6.fc.2.weight 38400\n",
      "features.25.conv.6.fc.2.bias 960\n",
      "features.25.conv.7.weight 153600\n",
      "features.25.conv.8.weight 160\n",
      "features.25.conv.8.bias 160\n",
      "features.26.conv.0.weight 153600\n",
      "features.26.conv.1.weight 960\n",
      "features.26.conv.1.bias 960\n",
      "features.26.conv.3.weight 8640\n",
      "features.26.conv.4.weight 960\n",
      "features.26.conv.4.bias 960\n",
      "features.26.conv.6.fc.0.weight 38400\n",
      "features.26.conv.6.fc.0.bias 40\n",
      "features.26.conv.6.fc.2.weight 38400\n",
      "features.26.conv.6.fc.2.bias 960\n",
      "features.26.conv.7.weight 245760\n",
      "features.26.conv.8.weight 256\n",
      "features.26.conv.8.bias 256\n",
      "features.27.conv.0.weight 393216\n",
      "features.27.conv.1.weight 1536\n",
      "features.27.conv.1.bias 1536\n",
      "features.27.conv.3.weight 13824\n",
      "features.27.conv.4.weight 1536\n",
      "features.27.conv.4.bias 1536\n",
      "features.27.conv.6.fc.0.weight 98304\n",
      "features.27.conv.6.fc.0.bias 64\n",
      "features.27.conv.6.fc.2.weight 98304\n",
      "features.27.conv.6.fc.2.bias 1536\n",
      "features.27.conv.7.weight 393216\n",
      "features.27.conv.8.weight 256\n",
      "features.27.conv.8.bias 256\n",
      "features.28.conv.0.weight 393216\n",
      "features.28.conv.1.weight 1536\n",
      "features.28.conv.1.bias 1536\n",
      "features.28.conv.3.weight 13824\n",
      "features.28.conv.4.weight 1536\n",
      "features.28.conv.4.bias 1536\n",
      "features.28.conv.6.fc.0.weight 98304\n",
      "features.28.conv.6.fc.0.bias 64\n",
      "features.28.conv.6.fc.2.weight 98304\n",
      "features.28.conv.6.fc.2.bias 1536\n",
      "features.28.conv.7.weight 393216\n",
      "features.28.conv.8.weight 256\n",
      "features.28.conv.8.bias 256\n",
      "features.29.conv.0.weight 393216\n",
      "features.29.conv.1.weight 1536\n",
      "features.29.conv.1.bias 1536\n",
      "features.29.conv.3.weight 13824\n",
      "features.29.conv.4.weight 1536\n",
      "features.29.conv.4.bias 1536\n",
      "features.29.conv.6.fc.0.weight 98304\n",
      "features.29.conv.6.fc.0.bias 64\n",
      "features.29.conv.6.fc.2.weight 98304\n",
      "features.29.conv.6.fc.2.bias 1536\n",
      "features.29.conv.7.weight 393216\n",
      "features.29.conv.8.weight 256\n",
      "features.29.conv.8.bias 256\n",
      "features.30.conv.0.weight 393216\n",
      "features.30.conv.1.weight 1536\n",
      "features.30.conv.1.bias 1536\n",
      "features.30.conv.3.weight 13824\n",
      "features.30.conv.4.weight 1536\n",
      "features.30.conv.4.bias 1536\n",
      "features.30.conv.6.fc.0.weight 98304\n",
      "features.30.conv.6.fc.0.bias 64\n",
      "features.30.conv.6.fc.2.weight 98304\n",
      "features.30.conv.6.fc.2.bias 1536\n",
      "features.30.conv.7.weight 393216\n",
      "features.30.conv.8.weight 256\n",
      "features.30.conv.8.bias 256\n",
      "features.31.conv.0.weight 393216\n",
      "features.31.conv.1.weight 1536\n",
      "features.31.conv.1.bias 1536\n",
      "features.31.conv.3.weight 13824\n",
      "features.31.conv.4.weight 1536\n",
      "features.31.conv.4.bias 1536\n",
      "features.31.conv.6.fc.0.weight 98304\n",
      "features.31.conv.6.fc.0.bias 64\n",
      "features.31.conv.6.fc.2.weight 98304\n",
      "features.31.conv.6.fc.2.bias 1536\n",
      "features.31.conv.7.weight 393216\n",
      "features.31.conv.8.weight 256\n",
      "features.31.conv.8.bias 256\n",
      "features.32.conv.0.weight 393216\n",
      "features.32.conv.1.weight 1536\n",
      "features.32.conv.1.bias 1536\n",
      "features.32.conv.3.weight 13824\n",
      "features.32.conv.4.weight 1536\n",
      "features.32.conv.4.bias 1536\n",
      "features.32.conv.6.fc.0.weight 98304\n",
      "features.32.conv.6.fc.0.bias 64\n",
      "features.32.conv.6.fc.2.weight 98304\n",
      "features.32.conv.6.fc.2.bias 1536\n",
      "features.32.conv.7.weight 393216\n",
      "features.32.conv.8.weight 256\n",
      "features.32.conv.8.bias 256\n",
      "features.33.conv.0.weight 393216\n",
      "features.33.conv.1.weight 1536\n",
      "features.33.conv.1.bias 1536\n",
      "features.33.conv.3.weight 13824\n",
      "features.33.conv.4.weight 1536\n",
      "features.33.conv.4.bias 1536\n",
      "features.33.conv.6.fc.0.weight 98304\n",
      "features.33.conv.6.fc.0.bias 64\n",
      "features.33.conv.6.fc.2.weight 98304\n",
      "features.33.conv.6.fc.2.bias 1536\n",
      "features.33.conv.7.weight 393216\n",
      "features.33.conv.8.weight 256\n",
      "features.33.conv.8.bias 256\n",
      "features.34.conv.0.weight 393216\n",
      "features.34.conv.1.weight 1536\n",
      "features.34.conv.1.bias 1536\n",
      "features.34.conv.3.weight 13824\n",
      "features.34.conv.4.weight 1536\n",
      "features.34.conv.4.bias 1536\n",
      "features.34.conv.6.fc.0.weight 98304\n",
      "features.34.conv.6.fc.0.bias 64\n",
      "features.34.conv.6.fc.2.weight 98304\n",
      "features.34.conv.6.fc.2.bias 1536\n",
      "features.34.conv.7.weight 393216\n",
      "features.34.conv.8.weight 256\n",
      "features.34.conv.8.bias 256\n",
      "features.35.conv.0.weight 393216\n",
      "features.35.conv.1.weight 1536\n",
      "features.35.conv.1.bias 1536\n",
      "features.35.conv.3.weight 13824\n",
      "features.35.conv.4.weight 1536\n",
      "features.35.conv.4.bias 1536\n",
      "features.35.conv.6.fc.0.weight 98304\n",
      "features.35.conv.6.fc.0.bias 64\n",
      "features.35.conv.6.fc.2.weight 98304\n",
      "features.35.conv.6.fc.2.bias 1536\n",
      "features.35.conv.7.weight 393216\n",
      "features.35.conv.8.weight 256\n",
      "features.35.conv.8.bias 256\n",
      "features.36.conv.0.weight 393216\n",
      "features.36.conv.1.weight 1536\n",
      "features.36.conv.1.bias 1536\n",
      "features.36.conv.3.weight 13824\n",
      "features.36.conv.4.weight 1536\n",
      "features.36.conv.4.bias 1536\n",
      "features.36.conv.6.fc.0.weight 98304\n",
      "features.36.conv.6.fc.0.bias 64\n",
      "features.36.conv.6.fc.2.weight 98304\n",
      "features.36.conv.6.fc.2.bias 1536\n",
      "features.36.conv.7.weight 393216\n",
      "features.36.conv.8.weight 256\n",
      "features.36.conv.8.bias 256\n",
      "features.37.conv.0.weight 393216\n",
      "features.37.conv.1.weight 1536\n",
      "features.37.conv.1.bias 1536\n",
      "features.37.conv.3.weight 13824\n",
      "features.37.conv.4.weight 1536\n",
      "features.37.conv.4.bias 1536\n",
      "features.37.conv.6.fc.0.weight 98304\n",
      "features.37.conv.6.fc.0.bias 64\n",
      "features.37.conv.6.fc.2.weight 98304\n",
      "features.37.conv.6.fc.2.bias 1536\n",
      "features.37.conv.7.weight 393216\n",
      "features.37.conv.8.weight 256\n",
      "features.37.conv.8.bias 256\n",
      "features.38.conv.0.weight 393216\n",
      "features.38.conv.1.weight 1536\n",
      "features.38.conv.1.bias 1536\n",
      "features.38.conv.3.weight 13824\n",
      "features.38.conv.4.weight 1536\n",
      "features.38.conv.4.bias 1536\n",
      "features.38.conv.6.fc.0.weight 98304\n",
      "features.38.conv.6.fc.0.bias 64\n",
      "features.38.conv.6.fc.2.weight 98304\n",
      "features.38.conv.6.fc.2.bias 1536\n",
      "features.38.conv.7.weight 393216\n",
      "features.38.conv.8.weight 256\n",
      "features.38.conv.8.bias 256\n",
      "features.39.conv.0.weight 393216\n",
      "features.39.conv.1.weight 1536\n",
      "features.39.conv.1.bias 1536\n",
      "features.39.conv.3.weight 13824\n",
      "features.39.conv.4.weight 1536\n",
      "features.39.conv.4.bias 1536\n",
      "features.39.conv.6.fc.0.weight 98304\n",
      "features.39.conv.6.fc.0.bias 64\n",
      "features.39.conv.6.fc.2.weight 98304\n",
      "features.39.conv.6.fc.2.bias 1536\n",
      "features.39.conv.7.weight 393216\n",
      "features.39.conv.8.weight 256\n",
      "features.39.conv.8.bias 256\n",
      "features.40.conv.0.weight 393216\n",
      "features.40.conv.1.weight 1536\n",
      "features.40.conv.1.bias 1536\n",
      "features.40.conv.3.weight 13824\n",
      "features.40.conv.4.weight 1536\n",
      "features.40.conv.4.bias 1536\n",
      "features.40.conv.6.fc.0.weight 98304\n",
      "features.40.conv.6.fc.0.bias 64\n",
      "features.40.conv.6.fc.2.weight 98304\n",
      "features.40.conv.6.fc.2.bias 1536\n",
      "features.40.conv.7.weight 393216\n",
      "features.40.conv.8.weight 256\n",
      "features.40.conv.8.bias 256\n",
      "conv.0.weight 458752\n",
      "conv.1.weight 1792\n",
      "conv.1.bias 1792\n",
      "classifier.weight 7168\n",
      "classifier.bias 4\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/d-li14/efficientnetv2.pytorch/blob/main/effnetv2.py\n",
    "import effnetv2 as bacha\n",
    "train_data_dir = os.path.abspath(\"./Training\")\n",
    "test_data_dir = os.path.abspath(\"./Testing\")\n",
    "model = bacha.effnetv2_s(num_classes=4).to(device)\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=3),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=train_data_dir, transform=transform)\n",
    "trainloader= torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "# Teacher training before starting the dataset distillation process\n",
    "epochs=30\n",
    "for epochy in range(epochs):\n",
    "    running_loss=0.0\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data) \n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    if (epochy+1)%10==0:\n",
    "        print('\\t',get_time(), 'Epoch: ', epochy+1 , 'Loss of model: ', running_loss/len(trainloader))\n",
    "\n",
    "model.eval()\n",
    "for param in list(model.parameters()):\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "del loss\n",
    "del data\n",
    "del output\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# test the accuracy of the model on train_dataloader\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in valloader:\n",
    "        images=images.to(device)\n",
    "        labels=labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_acc=100 * correct / total\n",
    "    print(f'\\t>> Accuracy of Model trained on Training set on testing set: {val_acc:.2f}%')\n",
    "\n",
    "\n",
    "\n",
    "# count the number of parameters of vision_net\n",
    "print(\"========Data Efficient Vision Transformer Architecture=========\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.numel())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torcher",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
